{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx-gZGRdb3I5"
      },
      "source": [
        "# Continuous GFlowNets on a Simple 1D Line Environment\n",
        "Joseph Viviano & Kolya Malkin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdZdAxVZb3I7"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from torch.distributions import Normal\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from tqdm import trange\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8IzCtpEb3I8"
      },
      "source": [
        "In this tutorial, we will explore a simple use-case of continuous GFlowNets: sampling from a distribution defined by a mixture of Gaussians. This is an exceedingly simple example which is not representative of the complexities inherent with applying this method in real applications, but will highlight some common challenges and useful tricks.\n",
        "\n",
        "First, **please run the cell below to define some helper functions and default hyperparameters:**\n",
        "\n",
        "# Run Me (below)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWU2aDQdb3I9",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def seed_all(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def render(reward, trajectory=None):\n",
        "    \"\"\"Renders the reward distribution over the 1D env.\"\"\"\n",
        "    x = np.linspace(\n",
        "        min(reward.mus) - reward.n_sd * max(reward.sigmas),\n",
        "        max(reward.mus) + reward.n_sd * max(reward.sigmas),\n",
        "        1000,\n",
        "    )\n",
        "\n",
        "    d = torch.exp(reward.log_reward(torch.tensor(x)))\n",
        "    dual_plot = not isinstance(trajectory, type(None))\n",
        "\n",
        "    if dual_plot:\n",
        "        fig, axs = plt.subplots(2, 1)\n",
        "        axs = axs.ravel()\n",
        "    else:\n",
        "        fig, axs = plt.subplots(1, 1)\n",
        "        axs = [axs]  # Hack to allow indexing.\n",
        "\n",
        "    if dual_plot:\n",
        "        ax_dual = axs[0].twinx()  # Second axes for final state counts.\n",
        "        ax_dual.hist(\n",
        "            trajectory[:, -1, 0].cpu().numpy(),  # Final X Position.\n",
        "            bins=100,\n",
        "            density=False,\n",
        "            alpha=0.5,\n",
        "            color=\"red\",\n",
        "        )\n",
        "        ax_dual.set_ylabel(\"Samples\", color=\"red\")\n",
        "        ax_dual.tick_params(axis=\"y\", labelcolor=\"red\")\n",
        "\n",
        "        n, trajectory_length, _ = trajectory.shape\n",
        "        for i in range(n):\n",
        "            axs[1].plot(\n",
        "                trajectory[i, :, 0].cpu().numpy(),\n",
        "                np.arange(1, trajectory_length + 1),\n",
        "                alpha=0.1,\n",
        "                linewidth=0.05,\n",
        "                color='black',\n",
        "            )\n",
        "            axs[1].set_ylabel('Step')\n",
        "\n",
        "    axs[0].plot(x, d, color=\"black\")\n",
        "\n",
        "    # Adds the modes.\n",
        "    for mu in reward.mus:\n",
        "        axs[0].axvline(mu, color=\"grey\", linestyle=\"--\")\n",
        "\n",
        "    # S0\n",
        "    axs[0].plot([reward.init_value], [0], 'ro')\n",
        "    axs[0].text(reward.init_value + 0.1, 0.01, \"$S_0$\", rotation=45)\n",
        "\n",
        "    # Means\n",
        "    for i, mu in enumerate(reward.mus):\n",
        "        idx = abs(x - mu.numpy()) == min(abs(x - mu.numpy()))\n",
        "        axs[0].plot([x[idx]], [d[idx]], 'bo')\n",
        "        axs[0].text(x[idx] + 0.1, d[idx], \"Mode {}\".format(i + 1), rotation=0)\n",
        "\n",
        "    axs[0].spines[['right', 'top']].set_visible(False)\n",
        "    axs[0].set_ylabel(\"Reward Value\")\n",
        "    axs[0].set_title(\"Line Environment\")\n",
        "    axs[0].set_ylim(0, max(d) * 1.1)\n",
        "\n",
        "    if dual_plot:\n",
        "        axs[1].set_xlim(axs[0].get_xlim())\n",
        "        axs[1].set_xlabel(\"X Position\")\n",
        "    else:\n",
        "        axs[0].set_xlabel(\"X Position\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Set up some fixed hyperparameters for the upcoming experiments\n",
        "# (until we need to change them).\n",
        "trajectory_length = 5\n",
        "min_policy_std = 0.1\n",
        "max_policy_std = 1.0\n",
        "batch_size = 256\n",
        "seed = 4444\n",
        "\n",
        "print(\"Beginning with the following hyperparameters:\")\n",
        "print(\"    trajectory_length: {}\".format(trajectory_length))\n",
        "print(\"    min_policy_std: {}\".format(min_policy_std))\n",
        "print(\"    max_policy_std: {}\".format(max_policy_std))\n",
        "print(\"    batch_size: {}\".format(batch_size))\n",
        "print(\"    seed: {}\".format(seed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg3aGgXNb3I-"
      },
      "source": [
        "Our task is simple: from an initial starting point on the number line, we want to sample a set of increments such that we ultimately sample from some reward distribution, defined as a mixture of Gaussians. Each increment (or step) will be sampled from Gaussians distributions as well.\n",
        "\n",
        "Continuous GFlowNets sample some real number(s) in a continuous space, in contrast to Discrete GFlowNets which sample discrete actions. Typically, this means your Continuous GFlowNet will use a function approximator $f(\\cdot)$, which accepts the current state $s_{t}$, to predict the *paramaters of a distribution* $\\rho = \\{p_1, p_2, ..., p_n\\}$, i.e. $\\rho = f(s_t)$.\n",
        "\n",
        "Then your chosen distribution(s) $D(\\rho)$ is used to sample a real-valued tensor $x \\sim D(\\rho)$ which is added to your current state to produce a the next step in the state space. This sampled $x$ can be used in many ways to compute the next state, for example, $s_{t+1} = x$; $s_{t+1} = s_{t} + x$; $s_{t+1} = s_{t} * x$; etc.\n",
        "\n",
        "> Note: we no longer consider a DAG here, but rather a topological space with distinguished source and sink states.\n",
        "\n",
        "For simplicity, we will be considering $x$ to be a *delta*, i.e., some relative change in the state. So in this example, we will be sampling $s_{\\Delta} \\sim D(\\rho)$ and computing $s_{t+1} = s_{t} + s_{\\Delta}$.\n",
        "\n",
        "In general, both $s_{\\Delta}$ and the distribution $D(\\rho)$ can be as complex as you want. In practice, this complexity can be tricky to handle and adds ample room for bugs, so these elements should be as simple as feasible for your problem. To get you started, we're going to work with $s_{\\Delta}$ being a single scalar, and $D(\\rho)$ being a simple Gaussian distribution. At the end, we will point to resources covering more complex settings which involve sampling from mixtures of distributions in $n$-dimensional spaces, which is more representative of common Continuous GFlowNet use-cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTqG41htb3I-"
      },
      "source": [
        "# Defining the Environment\n",
        "\n",
        "First let's define our environment. We require a few things. First, we need a reward distribution. This will be a mixture of Gaussians on the real number line, each defined by a $\\mu$ and $\\sigma$. Recall the formula of a Gaussian:\n",
        "\n",
        "$$g(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} exp \\big(-\\frac{1}{2} \\frac{(x-\\mu)^2}{\\sigma^2} \\big)$$\n",
        "\n",
        "The reward at each point will simply be the sum of the PDFs at that point across all elements of the mixture. We also define an arbitrary source state $S_0$ (where all trajectories will start) as `init_value`.\n",
        "\n",
        "The GFlowNet must sample increments along the number line such that it samples final values along the number line proportionally to the mixture distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsbOmDbEb3I-"
      },
      "outputs": [],
      "source": [
        "class LineEnvironment():\n",
        "    def __init__(self, mus, variances, n_sd, init_value):\n",
        "        self.mus = torch.tensor(mus)\n",
        "        self.sigmas = torch.tensor([math.sqrt(v) for v in variances])\n",
        "        self.variances = torch.tensor(variances)\n",
        "        self.mixture = [\n",
        "            Normal(m, s) for m, s in zip(self.mus, self.sigmas)\n",
        "        ]\n",
        "\n",
        "        self.n_sd = n_sd\n",
        "        self.lb = min(self.mus) - self.n_sd * max(self.sigmas)  # Convienience only.\n",
        "        self.ub = max(self.mus) + self.n_sd * max(self.sigmas)  # Convienience only.\n",
        "\n",
        "        self.init_value = init_value  # Used for s0.\n",
        "        assert self.lb < self.init_value < self.ub\n",
        "\n",
        "    def log_reward(self, x):\n",
        "        \"\"\"Log of the sum of the exponential of each log probability in the mixture.\"\"\"\n",
        "        return torch.logsumexp(torch.stack([m.log_prob(x) for m in self.mixture], 0), 0)\n",
        "\n",
        "    @property\n",
        "    def log_partition(self) -> float:\n",
        "        \"\"\"Log Partition is the log of the number of gaussians.\"\"\"\n",
        "        return torch.tensor(len(self.mus)).log()\n",
        "\n",
        "\n",
        "env = LineEnvironment(mus=[-1, 1], variances=[0.2, 0.2], n_sd=4.5, init_value=0)\n",
        "render(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnjiKwAib3I_"
      },
      "source": [
        "# Policies, Actions, and States\n",
        "\n",
        "Next, we need a distribution from which to sample these steps. To parameterize this, we will need a neural network to predict the parameters of the Gaussian: $\\mu$, the mean, and $\\sigma$, the standard deviation. We're also going to enforce that $ 0.1 <= \\sigma <= 1$ to help with convergence (see the hyperparameters above).\n",
        "\n",
        "Recall that in order to act in our environment, we have to do two things. First, we need to use a function approximator to predict the parameters of a distribution, then to sample from that distribution to produce the actions. Here, we have a neural network that outputs $\\mu$ and $\\sigma$ of the normal distribution, with the constraint that `min_policy_std <=` $\\sigma$ `<= max_policy_std`. Our policy is then this distribution object we can sample from:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADbA2owub3JA"
      },
      "outputs": [],
      "source": [
        "def get_policy_dist(model, x):\n",
        "    \"\"\"\n",
        "    A policy is a distribution we predict the parameters of using a neural network,\n",
        "    which we then sample from.\n",
        "    \"\"\"\n",
        "    pf_params = model(x)  # Shape = [batch_shape, 2].\n",
        "    policy_mean = pf_params[:, 0]\n",
        "    policy_std = torch.sigmoid(pf_params[:, 1]) * (max_policy_std - min_policy_std) + min_policy_std\n",
        "    policy_dist = torch.distributions.Normal(policy_mean, policy_std)\n",
        "\n",
        "    return policy_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFyV6pqgb3JA"
      },
      "source": [
        "## Why Our State Must Include A Counter\n",
        "\n",
        "We need to ensure there are no cycles in our state space to follow the theory of GFlowNets, but in this set up, a cycle would be easy to obtain. If we sampled first an increment of $+1$ and then an increment of $-1$, we could produce a cycle, and there are an infinite number of these on the real number line. To prevent cycles, let's simply include the count value, $t$, in the state $s_t$. In this setup, the state vector is `[x_position, n_steps]`, and the previous trajectory $[0, 0] \\rightarrow [1, 1] \\rightarrow [0, 2]$ would not be considerd a cycle. This step counter also can be used to know when to terminate this process, otherwise we never sample a final value. In this case, let's always terminate when $t=5$ (see hyperparameters above). There are more sophisitcated ways to handle termination, but they add complexity, and we want to focus this tutorial on only the core concepts.\n",
        "\n",
        "## Applying Actions to States\n",
        "\n",
        "For each forward action, we will add the action value to the current state, and increment the step counter. A backward action is simply the inverse: we will substract the action value from the current state, and decrement the *step* counter.\n",
        "\n",
        "Given this distribution we retrieve from `get_policy_dist()`, we sample an action $s_{\\Delta} \\sim D(\\rho)$. Recall that our state representation is `(x_position, n_steps)`. In this case, we are sampling $x_{\\Delta} \\sim \\mathcal{N}(\\mu, \\sigma^2)$, and our next state is `(x_position + x_delta, n_steps + 1)`.\n",
        "\n",
        "We'll also define a function that initalizes a state at $S_0$, which in our case has the `x_position` set to whatever we defined in our environment, and `n_steps` to 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlEPrqlqb3JA"
      },
      "outputs": [],
      "source": [
        "def step(x, action):\n",
        "    \"\"\"Takes a forward step in the environment.\"\"\"\n",
        "    new_x = torch.zeros_like(x)\n",
        "    ???  # TODO: Complete - add action delta.\n",
        "    ???  # TODO: Complete - increment step counter.\n",
        "\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def initalize_state(batch_size, device, env, randn=False):\n",
        "    \"\"\"Trajectory starts at state = (X_0, t=0).\"\"\"\n",
        "    x = torch.zeros((batch_size, 2), device=device)\n",
        "    ???  # TODO: Complete - add inital state.\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qazb2bySb3JA"
      },
      "source": [
        "# Model Definition, Training Loop, and Loss\n",
        "\n",
        "Below is a simple training loop. We're going to use Trajectory Balance (TB) for this demo. Recall the TB loss:\n",
        "\n",
        "$$L(\\tau) = \\left(\\log Z_{\\theta} + \\sum_t \\log P_F(s_{t+1}|s_t;\\theta) - \\log R(x) - \\sum_t \\log P_B(s_t|s_{t+1}; \\theta) \\right)^2$$\n",
        "\n",
        "so we need a forward model to generate trajectories $\\tau$ and calculate $P_F$, we need a backward model to calculate $P_F$, and we need an estimate of logZ $\\log Z_{\\theta}$, along with an optimizer.\n",
        "\n",
        "Let's set those up first. We'll parameterize both the `forward_model` and the `backward_model` as two small neural networks, each taking in the state `[x_position, n_steps]` and outputting the parameters of a Normal distribution `[mean, standard_deviation]`. The `logZ` estimate is a scalar parameter.\n",
        "\n",
        "> Note a common optimization trick for TB here, where the learning rate of `logZ` is faster than the other networks.\n",
        "\n",
        "These models will be used to predict the parameters of the policies themselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFkL8eQbb3JB"
      },
      "outputs": [],
      "source": [
        "def setup_experiment(hid_dim=64, lr_model=1e-3, lr_logz=1e-1):\n",
        "    \"\"\"Generate the learned parameters and optimizer for an experiment.\n",
        "\n",
        "    Forward and backward models are MLPs with a single hidden layer. logZ is\n",
        "    a single parameter. Note that we give logZ a higher learning rate, which is\n",
        "    a common trick used when utilizing Trajectory Balance.\n",
        "    \"\"\"\n",
        "    # Input = [x_position, n_steps], Output = [mus, standard_deviations].\n",
        "    forward_model = torch.nn.Sequential(torch.nn.Linear(2, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, 2)).to(device)\n",
        "\n",
        "    backward_model = torch.nn.Sequential(torch.nn.Linear(2, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, 2)).to(device)\n",
        "\n",
        "    logZ = torch.nn.Parameter(torch.tensor(0.0, device=device))\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        [\n",
        "            {'params': forward_model.parameters(), 'lr': lr_model},\n",
        "            {'params': backward_model.parameters(), 'lr': lr_model},\n",
        "            {'params': [logZ], 'lr': lr_logz},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (forward_model, backward_model, logZ, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0YviuLnb3JB"
      },
      "source": [
        "Finally, our training loop, which is greatly simplified due to the use of fixed-length trajectories (defined  as `trajectory_length` steps - currently set to 5). With probabilisitc exit actions, the logic becomes more tricky, though it is often useful.\n",
        "\n",
        "In our forward loop through the trajectory, we use our `forward_model` to calculate the `forward_policy`, sample an `action` from that policy, and finally calculate the log probability of that action an add it to `logPF`, which records the sum of the log probabilities (i.e., the product of the conditional probabilities, which is the probability of the trajectory). We store each new state in the trajectory.\n",
        "\n",
        "For the backward loop, we sample the parameters of a `backward_policy` from the `backward_model`, and evaluate the log probability of the observed action (`trajectory[:, t, 0] - trajectory[:, t - 1, 0]`) under this policy. The trajectory dimensions are `[batch_dim, trajectory_step, state]`. We use this to calculate the probability of the backward trajectory by accumulating the sum of `logPB`. Note that the `n_step` counter is only part of the state, not the action, so we can remove it when indexing on the `state` dimension.\n",
        "\n",
        "We finally calculate the `log_reward` from the terminal states under our environment reward, and calculate the trajectory balance loss, which we use to update the parameters of the `forward_model`, `backward_model`, and `logZ`.\n",
        "\n",
        "This should only take a few minutes to train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOX_3t9hb3JB"
      },
      "outputs": [],
      "source": [
        "def train(seed, batch_size, trajectory_length, env, device, n_iterations):\n",
        "    \"\"\"Continuous GFlowNet training loop, with the Trajectory Balance objective.\"\"\"\n",
        "    seed_all(seed)\n",
        "    forward_model, backward_model, logZ, optimizer = setup_experiment()  # Default hyperparameters used.\n",
        "    losses = []\n",
        "    tbar = trange(n_iterations)\n",
        "    true_logZ = env.log_partition\n",
        "\n",
        "    for it in tbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = initalize_state(batch_size, device, env)\n",
        "\n",
        "        # Trajectory stores all of the states in the forward loop.\n",
        "        trajectory = torch.zeros((batch_size, trajectory_length + 1, 2), device=device)\n",
        "        logPF = torch.zeros((batch_size,), device=device)\n",
        "        logPB = torch.zeros((batch_size,), device=device)\n",
        "\n",
        "        # Forward loop to generate full trajectory and compute logPF.\n",
        "        for t in range(trajectory_length):\n",
        "            policy_dist = ???  # TODO: Fill in the blank here.\n",
        "            action = ??? # TODO: Fill in the blank here.\n",
        "            logPF += ???  # TODO: Fill in the blank here.\n",
        "\n",
        "            new_x = step(x, action)\n",
        "            trajectory[:, t + 1, :] = new_x\n",
        "            x = new_x\n",
        "\n",
        "        # Backward loop to compute logPB from existing trajectory under the backward policy.\n",
        "        for t in range(trajectory_length, 0, -1):\n",
        "            policy_dist = ???  # TODO: Fill in the blank here.\n",
        "            action = ???  # TODO: Fill in the blank here.\n",
        "            logPB += ???  # TODO: Fill in the blank here.\n",
        "\n",
        "        log_reward = ???  # Reward from the final state.  # TODO: Fill in the blank here.\n",
        "\n",
        "        # Compute Trajectory Balance Loss.\n",
        "        loss = ???  # TODO: Fill in the blank here.\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if it % 100 == 0:\n",
        "            tbar.set_description(\"Training iter {}: (loss={:.3f}, estimated logZ={:.3f}, true logZ={:.3f}, LR={}\".format(\n",
        "                it,\n",
        "                np.array(losses[-100:]).mean(),\n",
        "                logZ.item(),\n",
        "                true_logZ,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return (forward_model, backward_model, logZ)\n",
        "\n",
        "n_iterations = 5000\n",
        "forward_model, backward_model, logZ = train(seed, batch_size, trajectory_length, env, device, n_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0hWcLcGb3JB"
      },
      "source": [
        "Next, we can view the behaviour of our GFlowNet by sampling new trajectories under fixed models. We'll plot two things. On top, we'll see the distribution of the final samples from each trajectory, alongside the original reward distribution. Underneath it, we'll show the `x_position` values at each step along the full trajectory. Note that at inference time, we only need the `forward_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ong0nN4mb3JB"
      },
      "outputs": [],
      "source": [
        "def inference(trajectory_length, forward_model, env, batch_size=10_000):\n",
        "    \"\"\"Sample some trajectories.\"\"\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        trajectory = torch.zeros((batch_size, trajectory_length + 1, 2), device=device)\n",
        "        trajectory[:, 0, 0] = env.init_value\n",
        "\n",
        "        x = initalize_state(batch_size, device, env)\n",
        "\n",
        "        for t in range(trajectory_length):\n",
        "            policy_dist = get_policy_dist(forward_model, x)\n",
        "            action = policy_dist.sample()\n",
        "\n",
        "            new_x = step(x, action)\n",
        "            trajectory[:, t + 1, :] = new_x\n",
        "            x = new_x\n",
        "\n",
        "    return trajectory\n",
        "\n",
        "trajectories = inference(trajectory_length, forward_model, env)\n",
        "render(env, trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Careful With that logPB, Eugene\n",
        "\n",
        "Yeesh! That doesn't look good. What happened? Recall that for the Trajectory Balance loss, we must match $P_F$ and $P_B$. When we sample our forward trajectory $\\tau_F = S_0 \\rightarrow S_1 \\rightarrow S_2 \\rightarrow \\cdots \\rightarrow S_4 \\rightarrow S_5$, we can accumulate the log probability of each transition under the gaussian distribution parameterized by our model. For the backward trajectory, $\\tau_B = S_5 \\rightarrow S_4 \\rightarrow \\cdots S_2 \\rightarrow S_1 \\rightarrow S_0$, we should accumulate the log probabilities using the same strategy, **except** for the transition $S_1 \\rightarrow S_0$.\n",
        "\n",
        "Since **every state reachable by the backward policy must also be reachable by the forward policy**, we also need to enforce that the the final transition of the backward policy goes exactly to $S_0$. Currently, we're sampling this transition, and under our current parameterization, it's actually impossible to precisely sample this exact transition every time. Since we **know** the transition $S_1 \\rightarrow S_0$ and that the backward trajectory must terminate at **exactly** $S_0$, we can say this transition happens with probability 1 (i.e., log probability 0). In the past implementation, we didn't do that, because our backward loop was `for t in range(trajectory_length, 0, -1)`...\n",
        "\n",
        "Let's fix this in the code below: if we make the backward loop `t in range(trajectory_length, 1, -1),`, we'll loop over a list of trajectory indices `[trajectory_length, trajectory_length-1, ..., 2]`, and the loop itself handles the transitions between `t` and `t-1`. Therefore, we will calculate the log probabilities of the transition $S_2 \\rightarrow S_1$, but not $S_1 \\rightarrow S_0$. This is correct, because that final transition happens with probability 1, and we should therefore correctly always add `0` when accumulating `logPB` for this last transition.\n"
      ],
      "metadata": {
        "id": "zvbYQ5ADfjsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_correct_PB(seed, batch_size, trajectory_length, env, device, n_iterations):\n",
        "    \"\"\"Continuous GFlowNet training loop, with the Trajectory Balance objective.\"\"\"\n",
        "    seed_all(seed)\n",
        "    forward_model, backward_model, logZ, optimizer = setup_experiment()  # Default hyperparameters used.\n",
        "    losses = []\n",
        "    tbar = trange(n_iterations)\n",
        "    true_logZ = env.log_partition\n",
        "\n",
        "    for it in tbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = initalize_state(batch_size, device, env)\n",
        "\n",
        "        # Trajectory stores all of the states in the forward loop.\n",
        "        trajectory = torch.zeros((batch_size, trajectory_length + 1, 2), device=device)\n",
        "        logPF = torch.zeros((batch_size,), device=device)\n",
        "        logPB = torch.zeros((batch_size,), device=device)\n",
        "\n",
        "        # Forward loop to generate full trajectory and compute logPF.\n",
        "        for t in range(trajectory_length):\n",
        "            policy_dist = get_policy_dist(forward_model, x)\n",
        "            action = policy_dist.sample()\n",
        "            logPF += policy_dist.log_prob(action)\n",
        "\n",
        "            new_x = step(x, action)\n",
        "            trajectory[:, t + 1, :] = new_x\n",
        "            x = new_x\n",
        "\n",
        "        # Backward loop to compute logPB from existing trajectory under the backward policy.\n",
        "        # Note this range skips S_1 -> S_0 -- we assign this action automatically p=1,\n",
        "        # or log probability 0, which we don't need to explictly accumulate to logPB.\n",
        "        for t in range(trajectory_length, 0, -1):  # TODO: Fix this line!\n",
        "            policy_dist = get_policy_dist(backward_model, trajectory[:, t, :])\n",
        "            action = trajectory[:, t, 0] - trajectory[:, t - 1, 0]\n",
        "            logPB += policy_dist.log_prob(action)\n",
        "\n",
        "        log_reward = env.log_reward(trajectory[:, -1, 0])  # Reward from the final state.\n",
        "\n",
        "        # Compute Trajectory Balance Loss.\n",
        "        loss = (logZ + logPF - logPB - log_reward).pow(2).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if it % 100 == 0:\n",
        "            tbar.set_description(\"Training iter {}: (loss={:.3f}, estimated logZ={:.3f}, true logZ={:.3f}, LR={}\".format(\n",
        "                it,\n",
        "                np.array(losses[-100:]).mean(),\n",
        "                logZ.item(),\n",
        "                true_logZ,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return (forward_model, backward_model, logZ)\n",
        "\n",
        "n_iterations = 5000\n",
        "forward_model, backward_model, logZ = train_with_correct_PB(seed, batch_size, trajectory_length, env, device, n_iterations)\n",
        "trajectories = inference(trajectory_length, forward_model, env)\n",
        "render(env, trajectories)"
      ],
      "metadata": {
        "id": "ohO8zGFZh0P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5g1dAcmb3JC"
      },
      "source": [
        "# I Have Dropped Modes and I Must Scream\n",
        "\n",
        "Well, it looks OK! Maybe not perfect (we could get there training longer), but let's move on. In this example, there's a reasonable amount of probability mass connecting the two modes of the distribution. Let's try to make the example harder by placing the modes far from $S_0$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKUevkb9b3JC"
      },
      "outputs": [],
      "source": [
        "env = LineEnvironment(mus=[-3, 3], variances=[0.2, 0.2], n_sd=4.5, init_value=0)\n",
        "render(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKu4cufkb3JC"
      },
      "source": [
        "Let's train a model using the same hyperparameters on this environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzqrQdcWb3JC"
      },
      "outputs": [],
      "source": [
        "n_iterations = 5000\n",
        "forward_model, backward_model, logZ = train_with_correct_PB(seed, batch_size, trajectory_length, env, device, n_iterations)\n",
        "trajectories = inference(trajectory_length, forward_model, env)\n",
        "render(env, trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGzpPzEvb3JD"
      },
      "source": [
        "What we're seeing here is mode collapse due to on policy training. We can fix this with off policy exploration.\n",
        "\n",
        "# Off Policy Exploration\n",
        "\n",
        "We can go off policy in many ways, but one simple way would be to add some constant to the variance predicted by our forward policy for the normal distribution. We can also decay this constant linearly over training iterations too facilitate convergence.\n",
        "\n",
        "Let's define a new function that retrieves both our learned policy and off policy sampler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYtvf7jqb3JD"
      },
      "outputs": [],
      "source": [
        "def get_policy_and_exploration_dist(model, x, off_policy_noise):\n",
        "    \"\"\"\n",
        "    A policy is a distribution we predict the parameters of using a neural network,\n",
        "    which we then sample from.\n",
        "    \"\"\"\n",
        "    pf_params = model(x)\n",
        "    policy_mean = pf_params[:, 0]\n",
        "    policy_std = torch.sigmoid(pf_params[:, 1]) * (max_policy_std - min_policy_std) + min_policy_std\n",
        "    policy_dist = torch.distributions.Normal(policy_mean, policy_std)\n",
        "\n",
        "    # Add some off-policy exploration.\n",
        "    exploration_dist = ???  # TODO: Fill in the blank here!\n",
        "\n",
        "    return policy_dist, exploration_dist\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyUVSrTwb3JD"
      },
      "source": [
        "In the below training loop, let's add the changes needed to allow for off policy exploration. To do so, we need to accomplish a few things:\n",
        "\n",
        "1. Define a value to increase the variance by, to encourage exploration. Ideally, this would be on a schedule, i.e,, the value we are adding to the variance of the predicted distribution will decrease over iterations. Let's use the `init_exploration_noise` variable for this.\n",
        "2. Sample actions from the exploration distribution.\n",
        "3. Calculate `logPF` using the log probabilities of the samples action from the policy distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX5_Cp-nb3JD"
      },
      "outputs": [],
      "source": [
        "def train_with_exploration(\n",
        "        seed,\n",
        "        batch_size,\n",
        "        trajectory_length,\n",
        "        env,\n",
        "        device,\n",
        "        init_exploration_noise,\n",
        "        n_iterations,\n",
        "    ):\n",
        "    \"\"\"Continuous GFlowNet training loop, with exploration, and the Trajectory Balance objective.\"\"\"\n",
        "    seed_all(seed)\n",
        "    forward_model, backward_model, logZ, optimizer = setup_experiment()   # Default hyperparameters used.\n",
        "    losses = []\n",
        "    tbar = trange(n_iterations)\n",
        "    true_logZ = env.log_partition\n",
        "\n",
        "    exploration_schedule = ???  # TODO: Fill in the blank, optional.\n",
        "\n",
        "    for iteration in tbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = initalize_state(batch_size, device, env)\n",
        "\n",
        "        # Trajectory stores all of the states in the forward loop.\n",
        "        trajectory = torch.zeros((batch_size, trajectory_length + 1, 2), device=device)\n",
        "        logPF = torch.zeros((batch_size,), device=device)\n",
        "        logPB = torch.zeros((batch_size,), device=device)\n",
        "\n",
        "        # Forward loop to generate full trajectory and compute logPF.\n",
        "        for t in range(trajectory_length):\n",
        "            # TODO: Fill in the blanks here.\n",
        "            policy_dist, exploration_dist = get_policy_and_exploration_dist(???) # TODO: Fill in the blank here.\n",
        "            action = ???  # TODO: Fill in the blank here.\n",
        "            logPF += ???  # TODO: Fill in the blank here.\n",
        "\n",
        "            new_x = step(x, action)\n",
        "            trajectory[:, t + 1, :] = new_x\n",
        "            x = new_x\n",
        "\n",
        "        # Backward loop to compute logPB from existing trajectory under the backward policy.\n",
        "        # Note this range skips S_1 -> S_0 -- we assign this action automatically p=1,\n",
        "        # or log probability 0, which we don't need to explictly accumulate to logPB.\n",
        "        for t in range(trajectory_length, 1, -1):\n",
        "            policy_dist = get_policy_dist(backward_model, trajectory[:, t, :])\n",
        "            action = trajectory[:, t, 0] - trajectory[:, t - 1, 0]\n",
        "            logPB += policy_dist.log_prob(action)\n",
        "\n",
        "        log_reward = env.log_reward(trajectory[:, -1, 0])  # Reward from the final state.\n",
        "\n",
        "        # Compute Trajectory Balance Loss.\n",
        "        loss = (logZ + logPF - logPB - log_reward).pow(2).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            tbar.set_description(\"Training iter {}: (loss={:.3f}, estimated logZ={:.3f}, true logZ={:.3f}, LR={})\".format(\n",
        "                iteration,\n",
        "                np.array(losses[-100:]).mean(),\n",
        "                logZ.item(),\n",
        "                true_logZ,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return (forward_model, backward_model, logZ)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RmSaztEb3JD"
      },
      "source": [
        "Note there are 3 elements defining the difficulty of learning this task: the `init_exploration_noise`, `trajectory_length`, and `n_iterations` hyperparameters. Increasing the noise too much without increasing the trajectory length or , or vice versa, will not produce a good solution. We've provided good hyperparameters here, but feel free to play around to see how the hyperparameters interact."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M-BAgDMb3JD"
      },
      "outputs": [],
      "source": [
        "init_exploration_noise = 2\n",
        "trajectory_length = 5\n",
        "n_iterations = 5_000\n",
        "\n",
        "forward_model, backward_model, logZ = train_with_exploration(\n",
        "    seed,\n",
        "    batch_size,\n",
        "    trajectory_length,\n",
        "    env,\n",
        "    device,\n",
        "    init_exploration_noise,\n",
        "    n_iterations,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkoy5jrEb3JE"
      },
      "outputs": [],
      "source": [
        "trajectories = inference(trajectory_length, forward_model, env)\n",
        "render(env, trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTnHOs-7b3JE"
      },
      "source": [
        "# A Too Hard Example?\n",
        "\n",
        "Looks good! Let's try something more complex..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOFzrw7lb3JE"
      },
      "outputs": [],
      "source": [
        "env = LineEnvironment(\n",
        "    mus=[-2, 4, 6, 10],\n",
        "    variances=[0.2, 0.4, 1, 0.2],\n",
        "    n_sd=4.5,\n",
        "    init_value=0\n",
        ")\n",
        "render(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BbuArtRb3JE"
      },
      "source": [
        "Here, we have 4 modes, and our starting point $S_0$ is now closest to Mode 1, and the different modes have different reward values. Let's train for 10k iterations to help the model out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps3gSkwOb3JE"
      },
      "outputs": [],
      "source": [
        "init_exploration_noise = 2\n",
        "trajectory_length = 5\n",
        "n_iterations = 10_000\n",
        "min_policy_std = 0.1\n",
        "max_policy_std = 1.0\n",
        "\n",
        "forward_model, backward_model, logZ = train_with_exploration(\n",
        "    seed,\n",
        "    batch_size,\n",
        "    trajectory_length,\n",
        "    env,\n",
        "    device,\n",
        "    init_exploration_noise,\n",
        "    n_iterations=n_iterations,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCIyWb0-b3JF"
      },
      "outputs": [],
      "source": [
        "trajectories = inference(trajectory_length, forward_model, env)\n",
        "render(env, trajectories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86usYDqOb3JF"
      },
      "source": [
        "Things aren't looking good yet. The model has learned to x positions reliabily in the highest reward regions, but it isn't modelling mode 2 and 3 well yet, so we are nowhere close to sampling from the correct probability distribution.\n",
        "\n",
        "We can get decent results with more compute: `trajectory_lengths=10` and `n_iterations=50_000`, but we don't have time to train this during the tutorial. Try playing with this notebook at home to build an intution as to how the different hyperparameters affect the results:\n",
        "\n",
        "+ `trajectory_length`\n",
        "+ `init_exploration_noise`\n",
        "+ `min/max_policy_std`\n",
        "+ `n_iterations`\n",
        "+ `learning_rate` (for the model and logZ estimate seperately!)\n",
        "+ `hid_dim`\n",
        "\n",
        "A key takeaway here is that the complexity of tuning the hyperparameters for training a continuous GFlowNet quickly grows with the complexity of the environment... even in a very simple case such as this one.\n",
        "\n",
        "# Harder (and More Realistic) Examples in the `GFlowNet` Library\n",
        "\n",
        "For a more interesting example, please see [the Continuous Cube Environment](https://github.com/alexhernandezgarcia/gflownet/blob/main/gflownet/envs/cube.py#L286) from [`GFlowNet`](https://github.com/alexhernandezgarcia/gflownet/tree/main), a GFlowNet training framework designed by a group of researchers at Mila (including a few of this workshop's instructors: Alex Hernandez-Garcia, Micha≈Ç Koziarski, Victor Schmidt, and Alexandra Volokhova).\n",
        "\n",
        "For installation, see [here](https://github.com/alexhernandezgarcia/gflownet/tree/main) and see the configuration file [here](https://github.com/alexhernandezgarcia/gflownet/tree/main/config/experiments/ccube) to train the task on your machine. This environment implements multiple important complexities we have so far ignored:\n",
        "\n",
        "+ The environment allows for early termination by including a [Bernoulli in the action space](https://github.com/alexhernandezgarcia/gflownet/blob/96f1eaf44e1200bd45ec4a6a5a8a5acca8ff0111/gflownet/envs/cube.py#L360) which represents the probability of exiting.\n",
        "+ The action space is governed by Beta distribution (i.e., with support on $(0, 1)$) with a scale parameter.\n",
        "+ Forward and backward actions are [governed by different distributions](https://github.com/alexhernandezgarcia/gflownet/blob/96f1eaf44e1200bd45ec4a6a5a8a5acca8ff0111/gflownet/envs/cube.py#L593). This is important: in general, forward and backward actions **are not the same** when defining Continuous GFlowNets.\n",
        "+ The log probability calculations with a mixture of dsitributions becomes [complicated very quickly](https://github.com/alexhernandezgarcia/gflownet/blob/96f1eaf44e1200bd45ec4a6a5a8a5acca8ff0111/gflownet/envs/cube.py#L940), especially if the calculation is step-dependent.\n",
        "\n",
        "This is only scratching the surface, but I hope it is clear from glancing at this implementation how Continuous GFlowNets quickly become challenging to handle, so using them for non-toy applications should be done with careful consideration. There is still a lot of research to be done on improving their ease-of-use!\n",
        "\n",
        "# A Self Contained Example of the SImple 1D Environment\n",
        "\n",
        "For your convienience, we've copied all relevant logic from the above here, so you can play with the hyperparameters etc. to see how this affects training of the Continuous GFlowNet. **For this to work, please remember to run the imports and helper function cells at the beginning of the notebook!**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LineEnvironment():\n",
        "    def __init__(self, mus, variances, n_sd, init_value):\n",
        "        self.mus = torch.tensor(mus)\n",
        "        self.sigmas = torch.tensor([math.sqrt(v) for v in variances])\n",
        "        self.variances = torch.tensor(variances)\n",
        "        self.mixture = [\n",
        "            Normal(torch.tensor(m), torch.tensor(s)) for m, s in zip(mus, self.sigmas)\n",
        "        ]\n",
        "\n",
        "        self.n_sd = n_sd\n",
        "        self.lb = min(self.mus) - self.n_sd * max(self.sigmas)  # Convienience only.\n",
        "        self.ub = max(self.mus) + self.n_sd * max(self.sigmas)  # Convienience only.\n",
        "\n",
        "        self.init_value = init_value  # Used for s0.\n",
        "        assert self.lb < self.init_value < self.ub\n",
        "\n",
        "    def log_reward(self, x):\n",
        "        \"\"\"Sum of the exponential of each log probability in the mixture.\"\"\"\n",
        "        return torch.logsumexp(torch.stack([m.log_prob(x) for m in self.mixture], 0), 0)\n",
        "\n",
        "    @property\n",
        "    def log_partition(self) -> float:\n",
        "        \"\"\"Log Partition is the log of the number of gaussians.\"\"\"\n",
        "        return torch.tensor(len(self.mus)).log()\n",
        "\n",
        "\n",
        "def setup_experiment(hid_dim=64, lr_model=1e-3, lr_logz=1e-1):\n",
        "    \"\"\"Generate the learned parameters and optimizer for an experiment.\n",
        "\n",
        "    Forward and backward models are MLPs with a single hidden layer. logZ is\n",
        "    a single parameter. Note that we give logZ a higher learning rate, which is\n",
        "    a common trick used when utilizing Trajectory Balance.\n",
        "    \"\"\"\n",
        "    # Input = [x_position, n_steps], Output = [mus, standard_deviations].\n",
        "    forward_model = torch.nn.Sequential(torch.nn.Linear(2, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, 2)).to(device)\n",
        "\n",
        "    backward_model = torch.nn.Sequential(torch.nn.Linear(2, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, hid_dim),\n",
        "                                        torch.nn.ELU(),\n",
        "                                        torch.nn.Linear(hid_dim, 2)).to(device)\n",
        "\n",
        "    logZ = torch.nn.Parameter(torch.tensor(0.0, device=device))\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        [\n",
        "            {'params': forward_model.parameters(), 'lr': lr_model},\n",
        "            {'params': backward_model.parameters(), 'lr': lr_model},\n",
        "            {'params': [logZ], 'lr': lr_logz},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (forward_model, backward_model, logZ, optimizer)\n",
        "\n",
        "\n",
        "def step(x, action):\n",
        "    \"\"\"Takes a forward step in the environment.\"\"\"\n",
        "    new_x = torch.zeros_like(x)\n",
        "    new_x[:, 0] = x[:, 0] + action  # Add action delta.\n",
        "    new_x[:, 1] = x[:, 1] + 1  # Increment step counter.\n",
        "\n",
        "    return new_x\n",
        "\n",
        "\n",
        "def initalize_state(batch_size, device, env, randn=False):\n",
        "    \"\"\"Trajectory starts at state = (X_0, t=0).\"\"\"\n",
        "    x = torch.zeros((batch_size, 2), device=device)\n",
        "    x[:, 0] = env.init_value\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def get_policy_and_exploration_dist(model, x, off_policy_noise):\n",
        "    \"\"\"\n",
        "    A policy is a distribution we predict the parameters of using a neural network,\n",
        "    which we then sample from.\n",
        "    \"\"\"\n",
        "    pf_params = model(x)\n",
        "    policy_mean = pf_params[:, 0]\n",
        "    policy_std = torch.sigmoid(pf_params[:, 1]) * (max_policy_std - min_policy_std) + min_policy_std\n",
        "    policy_dist = torch.distributions.Normal(policy_mean, policy_std)\n",
        "\n",
        "    # Add some off-policy exploration.\n",
        "    exploration_dist = torch.distributions.Normal(policy_mean, policy_std + off_policy_noise)\n",
        "\n",
        "    return policy_dist, exploration_dist\n",
        "\n",
        "\n",
        "def inference(trajectory_length, forward_model, env, batch_size=10_000):\n",
        "    \"\"\"Sample some trajectories.\"\"\"\n",
        "    with torch.no_grad():\n",
        "        trajectory = torch.zeros((batch_size, trajectory_length + 1, 2), device=device)\n",
        "        trajectory[:, 0, 0] = env.init_value\n",
        "\n",
        "        x = initalize_state(batch_size, device, env)\n",
        "\n",
        "        for t in range(trajectory_length):\n",
        "            policy_dist, _ = get_policy_and_exploration_dist(forward_model, x, 0)\n",
        "            action = policy_dist.sample()\n",
        "\n",
        "            new_x = step(x, action)\n",
        "            trajectory[:, t + 1, :] = new_x\n",
        "            x = new_x\n",
        "\n",
        "    return trajectory\n",
        "\n",
        "\n",
        "def train_with_exploration(\n",
        "        seed,\n",
        "        batch_size,\n",
        "        trajectory_length,\n",
        "        env,\n",
        "        device,\n",
        "        init_explortation_noise,\n",
        "        n_iterations=10_000\n",
        "    ):\n",
        "    \"\"\"Continuous GFlowNet training loop, with exploration, and the Trajectory Balance objective.\"\"\"\n",
        "    seed_all(seed)\n",
        "    forward_model, backward_model, logZ, optimizer = setup_experiment()   # Default hyperparameters used.\n",
        "    losses = []\n",
        "    tbar = trange(n_iterations)\n",
        "    true_logZ = env.log_partition\n",
        "\n",
        "    exploration_schedule = np.linspace(init_explortation_noise, 0,  n_iterations)\n",
        "\n",
        "    for iteration in tbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = initalize_state(batch_size, device, env)\n",
        "\n",
        "        # Trajectory stores all of the states in the forward loop.\n",
        "        trajectory = torch.zeros((batch_size, trajectory_length + 1, 2), device=device)\n",
        "        logPF = torch.zeros((batch_size,), device=device)\n",
        "        logPB = torch.zeros((batch_size,), device=device)\n",
        "\n",
        "        # Forward loop to generate full trajectory and compute logPF.\n",
        "        for t in range(trajectory_length):\n",
        "\n",
        "            policy_dist, exploration_dist = get_policy_and_exploration_dist(\n",
        "                forward_model,\n",
        "                x,\n",
        "                exploration_schedule[iteration],\n",
        "            )\n",
        "            action = exploration_dist.sample()\n",
        "            logPF += policy_dist.log_prob(action)\n",
        "\n",
        "            new_x = step(x, action)\n",
        "            trajectory[:, t + 1, :] = new_x\n",
        "            x = new_x\n",
        "\n",
        "        # Backward loop to compute logPB from existing trajectory under the backward policy.\n",
        "        # Note this range skips S_1 -> S_0 -- we assign this action automatically p=1,\n",
        "        # or log probability 0, which we don't need to explictly accumulate to logPB.\n",
        "        for t in range(trajectory_length, 1, -1):\n",
        "            policy_dist, _ = get_policy_and_exploration_dist(\n",
        "                backward_model,\n",
        "                trajectory[:, t, :],\n",
        "                1,  # Exploration dist isn't used.\n",
        "            )\n",
        "            action = trajectory[:, t, 0] - trajectory[:, t - 1, 0]\n",
        "            logPB += policy_dist.log_prob(action)\n",
        "\n",
        "        log_reward = env.log_reward(trajectory[:, -1, 0])  # Reward from the final state.\n",
        "\n",
        "        # Compute Trajectory Balance Loss.\n",
        "        loss = (logZ + logPF - logPB - log_reward).pow(2).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if iteration % 100 == 0:\n",
        "            tbar.set_description(\"Training iter {}: (loss={:.3f}, estimated logZ={:.3f}, true logZ={:.3f}, LR={}, off policy noise={:.4f})\".format(\n",
        "                iteration,\n",
        "                np.array(losses[-100:]).mean(),\n",
        "                logZ.item(),\n",
        "                true_logZ,\n",
        "                optimizer.param_groups[0]['lr'],\n",
        "                exploration_schedule[iteration],\n",
        "                )\n",
        "            )\n",
        "\n",
        "    return (forward_model, backward_model, logZ)\n",
        "\n",
        "\n",
        "# Hyperparameters.\n",
        "batch_size = 256\n",
        "init_exploration_noise = 5\n",
        "max_policy_std = 5.0\n",
        "min_policy_std = 0.1\n",
        "n_iterations = 10_000\n",
        "seed = 4444\n",
        "trajectory_length = 5\n",
        "\n",
        "# Define Environment.\n",
        "env = LineEnvironment(\n",
        "    mus=[-2, 4, 6, 10],\n",
        "    variances=[0.2, 0.4, 1, 0.2],\n",
        "    n_sd=4.5,\n",
        "    init_value=0\n",
        ")\n",
        "\n",
        "\n",
        "# Train.\n",
        "forward_model, backward_model, logZ = train_with_exploration(\n",
        "    seed,\n",
        "    batch_size,\n",
        "    trajectory_length,\n",
        "    env,\n",
        "    device,\n",
        "    init_exploration_noise,\n",
        "    n_iterations=n_iterations,\n",
        ")\n",
        "\n",
        "# Plot Results.\n",
        "trajectories = inference(trajectory_length, forward_model, env)\n",
        "render(env, trajectories)\n"
      ],
      "metadata": {
        "id": "Y-yX85oRmEMq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vae",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}