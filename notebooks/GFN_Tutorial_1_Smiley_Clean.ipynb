{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrLt_YA1nQLO"
      },
      "source": [
        "# GFlowNets 101: Learning to Sample Happiness\n",
        "Joseph Viviano & Emmanuel Bengio\n",
        "\n",
        "(This material was adapted from [Emmanuel Bengio's 2022 Tutorial](https://colab.research.google.com/drive/1fUMwgu2OhYpQagpzU5mhe9_Esib3Q2VR#scrollTo=w51Ha8TRZ29Z))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hagz-Jq3JKvj"
      },
      "outputs": [],
      "source": [
        "# https://pypi.python.org/pypi/torchgfn  # currently pulling from\n",
        "# `easier_environment_definition` until we do the next release of torchgfn.\n",
        "%%capture\n",
        "!pip uninstall tensorflow -y && pip install torchtyping\n",
        "!pip install git+https://github.com/GFNOrg/torchgfn.git@easier_environment_definition\n",
        "!pip install typing-extensions --upgrade\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.cm as cm\n",
        "import random\n",
        "\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihLL2qXwbWF6"
      },
      "source": [
        "GFlowNets (GFNs) are a generative modelling framwork - we train a model to sample from $p(x)$, i.e., we amortize the cost of sampling into the training process. This means we're going to have a `.sample()` method somewhere to generate an object $x$.\n",
        "\n",
        "The most common and easiest use-case of a GFN is to sample compositional, discrete objects like sets, sequences, or graphs. The objects are sampled piece-by-piece, i.e., the `.sample()` method iterates over $n$-steps of a `for` loop to generate an object composed of $n$ components.\n",
        "\n",
        "GFNs are different from \"dataset-based\" methods (such as variational inference), because we assume instead we have access to an unnormalized density function $R(x)$ which attributes a value to every generated $x$ (a dataset might come into play if $R(x)$ is the result of a model trained on some dataset $D$, but the GFN can be trained using rewards calculated on sampled objects $x \\notin D$).\n",
        "\n",
        "GFNs are different from vanilla RL methods in that wee use the reward to learn $p(x)$, instead of a reward maximizing policy $\\pi$. In other words, we seek $p(x) \\propto R(x)$.\n",
        "\n",
        "But first, we're going to define some helper functions (hidden). Make sure to run this cell! Once complete, it will print out the hyperparameters for our models.\n",
        "\n",
        "# Run Me (below!)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# These feature globals will be referred to throughout.\n",
        "_mouth_kwargs = {\"closed\": False, \"fill\": False, \"lw\": 3}\n",
        "FEATURES = {\n",
        "    'smile': lambda: plt.gca().add_patch(plt.Polygon(\n",
        "        np.stack(\n",
        "            [np.linspace(0.2, 0.8), 0.3 - np.sin(np.linspace(0, 3.14)) * 0.15]\n",
        "        ).T,\n",
        "        **_mouth_kwargs\n",
        "        )\n",
        "    ),\n",
        "    'frown': lambda: plt.gca().add_patch(plt.Polygon(\n",
        "        np.stack(\n",
        "            [np.linspace(0.2, 0.8), 0.15 + np.sin(np.linspace(0, 3.14)) * 0.15]\n",
        "        ).T,\n",
        "        **_mouth_kwargs,\n",
        "        )\n",
        "    ),\n",
        "    'left_eb_down': lambda: plt.gca().add_line(plt.Line2D(\n",
        "        [0.15, 0.35], [0.75, 0.7], color=(0, 0, 0))\n",
        "    ),\n",
        "    'right_eb_down': lambda: plt.gca().add_line(plt.Line2D(\n",
        "        [0.65, 0.85], [0.7, 0.75], color=(0, 0, 0))\n",
        "    ),\n",
        "    'left_eb_up': lambda: plt.gca().add_line(plt.Line2D(\n",
        "        [0.15, 0.35], [0.7, 0.75], color=(0, 0, 0))\n",
        "    ),\n",
        "    'right_eb_up': lambda: plt.gca().add_line(plt.Line2D(\n",
        "        [0.65, 0.85], [0.75, 0.7], color=(0, 0, 0))\n",
        "    ),\n",
        "}\n",
        "\n",
        "\n",
        "def draw_face(face, title):\n",
        "    \"\"\"Given a list of features, render a face to an axis.\"\"\"\n",
        "    # Draw the yellow circle & two eyes.\n",
        "    plt.gca().add_patch(plt.Circle((0.5, 0.5), 0.5, fc=(.9,.9,0))),\n",
        "    plt.gca().add_patch(plt.Circle((0.25, 0.6), 0.1, fc=(0,0,0))),\n",
        "    plt.gca().add_patch(plt.Circle((0.75, 0.6), 0.1,  fc=(0,0,0)))\n",
        "\n",
        "    for feat in face:\n",
        "        FEATURES[feat]()\n",
        "\n",
        "    plt.axis('scaled')\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        plt.gca().set_title(title)\n",
        "\n",
        "\n",
        "# Some utility functions:\n",
        "def plot_loss_curve(losses_A, losses_B=None, title=\"\"):\n",
        "\n",
        "    plt.figure(figsize=(10,3))\n",
        "\n",
        "    if isinstance(losses_B, type(None)):\n",
        "        plt.plot(losses, color=\"black\")\n",
        "    else:\n",
        "        plt.plot(losses_A, color=\"blue\", linewidth=1, label=\"No Forward Masks\")\n",
        "        plt.plot(losses_B, color=\"red\", linewidth=1, label=\"Forward Masks\", alpha=0.5)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "\n",
        "\n",
        "def check_sampled_faces(sampled_faces):\n",
        "\n",
        "    fig, ax = plt.subplots(8, 8, figsize=(4,4))\n",
        "\n",
        "    n = 128  # final n samples to calculate stats over.\n",
        "    n_plot = 64  # 8 x 8\n",
        "    print('Ratio of faces with a smile:{}, ideal={}'.format(\n",
        "        sum(['smile' in i for i in sampled_faces[-n:]]) / n,\n",
        "        2/3.,\n",
        "    ))\n",
        "    print('Proportion of valid faces:{}, ideal=1'.format(\n",
        "        sum([face_reward(i) > 0 for i in sampled_faces[-n:]]) / n\n",
        "    ))\n",
        "\n",
        "    for i, face in enumerate(sampled_faces[-n_plot:]):\n",
        "      plt.sca(ax[i//8, i%8])\n",
        "      draw_face(face, \"\")\n",
        "\n",
        "\n",
        "def analyze_torchgfn_results(visited_terminating_states, env, n_samples = 128):\n",
        "\n",
        "    env = FacesEnv()\n",
        "\n",
        "    # Calculate sample stats.\n",
        "    n_samples = 128\n",
        "    smiles = visited_terminating_states[-n_samples:].tensor[:, -2]\n",
        "    frowns = visited_terminating_states[-n_samples:].tensor[:, -1]\n",
        "    print(\"proportion of smiles:frowns = {}:{}\".format(\n",
        "        torch.mean(smiles),\n",
        "        torch.mean(frowns),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Generate smilies in string format.\n",
        "    sampled_faces = []\n",
        "    for s in visited_terminating_states[-n_samples:]:\n",
        "        face = []\n",
        "        for i, elem in enumerate(s.tensor.tolist()):\n",
        "            if elem == 1:\n",
        "                face.append(env.feature_keys[i])\n",
        "        sampled_faces.append(face)\n",
        "\n",
        "    # Plot smilies.\n",
        "    fig, ax = plt.subplots(8, 8, figsize=(4, 4))\n",
        "    n_plot = 64  # 8 x 8\n",
        "    for i, face in enumerate(sampled_faces[-n_plot:]):\n",
        "        plt.sca(ax[i//8, i%8])\n",
        "        draw_face(face, \"\")\n",
        "\n",
        "\n",
        "def recursively_enumerate(s, enumerated_states, transitions, only_valid=True):\n",
        "\n",
        "    # Detects invalid configurations.\n",
        "    if only_valid:\n",
        "        if has_overlap(s):\n",
        "            return\n",
        "    else:\n",
        "        if len(set(s)) < len(s):  # Don't allow for duplicates.\n",
        "\n",
        "            return\n",
        "        elif len(s) > 3:  # Max trajectory length.\n",
        "            return\n",
        "\n",
        "    for i in FEATURE_KEYS:\n",
        "        if i not in s:\n",
        "            recursively_enumerate(\n",
        "                s + [i],\n",
        "                enumerated_states,\n",
        "                transitions,\n",
        "                only_valid=only_valid\n",
        "            )\n",
        "\n",
        "    enumerated_states.append(s)\n",
        "    transitions.append((s[:-1], s))\n",
        "\n",
        "\n",
        "def get_unique(l: list):\n",
        "\n",
        "    unique = []\n",
        "    for i in map(set, l):\n",
        "      if i not in unique:\n",
        "        unique.append(i)\n",
        "\n",
        "    return sorted(map(tuple, unique))\n",
        "\n",
        "\n",
        "def get_face_positions_and_plot(enumerated_states, transitions, F_sa = None):\n",
        "    \"\"\"Plot a graph of all state transitions.\n",
        "\n",
        "    If a GFlowNet state flow estimator if provided, the edges of the graph will be\n",
        "    colors according to the flows.\n",
        "    \"\"\"\n",
        "    use_flows = True if not isinstance(F_sa, type(None)) else False\n",
        "\n",
        "    lens, levels = [], []\n",
        "    face2pos = {}\n",
        "\n",
        "    for i in range(4):  # All valid trajectories are length 4 (including s0).\n",
        "        lens.append(len([j for j in enumerated_states if len(j) == i]))\n",
        "        levels.append(sorted([j for j in enumerated_states if len(j) == i]))\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "    # Maps from unique face hashes to unique (x,y) coorindates for plotting.\n",
        "    for i, (level, L) in enumerate(zip(levels, lens)):\n",
        "        for j, face in enumerate(level):\n",
        "            ax = fig.add_axes([j/L, i/4, 1/L, 1/6])\n",
        "            draw_face(face, \"\")\n",
        "            face2pos[face_hash(face)] = (j/L + 0.5/L, i/4)\n",
        "\n",
        "    ax = fig.add_axes([0, 0, 1, 1])\n",
        "    plt.sca(ax)\n",
        "    plt.gca().set_facecolor((0, 0, 0, 0))\n",
        "    plt.xlim(0,1)\n",
        "    plt.ylim(0,1)\n",
        "\n",
        "    # Now add all transitions (arrows).\n",
        "    if use_flows:\n",
        "      max_flow = 0\n",
        "\n",
        "    for face_a, face_b in transitions[1:]:\n",
        "\n",
        "        if not len(face_b):  # We're at the end of the trajectory.\n",
        "          continue\n",
        "\n",
        "        pos_a, pos_b = face2pos[face_hash(face_a)], face2pos[face_hash(face_b)]\n",
        "\n",
        "        # Optionally color the arrows by the learned flows.\n",
        "        if use_flows:\n",
        "            Fstate = F_sa(face_to_tensor(face_a))  # State flow.\n",
        "\n",
        "            Fa = Fstate[\n",
        "                FEATURE_KEYS.index([i for i in face_b if i not in face_a][0])\n",
        "            ].item()\n",
        "            c = cm.Reds(Fa)\n",
        "\n",
        "            if Fa > max_flow:\n",
        "              max_flow = Fa\n",
        "\n",
        "        la = int(pos_a[1] * 4)\n",
        "        lb = int(pos_b[1] * 4)\n",
        "        ws = [1/6, 1/6, 0.13, 0.11]\n",
        "        plt.arrow(\n",
        "            pos_a[0],\n",
        "            pos_a[1] + ws[la],\n",
        "            pos_b[0] - pos_a[0],\n",
        "            pos_b[1] - pos_a[1] - ws[lb],\n",
        "            head_width=0.01,\n",
        "            width=0.003,\n",
        "            ec=c if use_flows else (1, 0, 0),\n",
        "            fc=c if use_flows else (0, 0, 0),\n",
        "            length_includes_head=True,\n",
        "        )\n",
        "        plt.axis('off')\n",
        "\n",
        "    if use_flows:\n",
        "        ax = fig.add_axes([1, 0.2, 0.05, 0.6])\n",
        "        plt.sca(ax)\n",
        "        fig.colorbar(\n",
        "            cm.ScalarMappable(\n",
        "                norm=cm.colors.Normalize(vmin=0, vmax=max_flow),\n",
        "                cmap=cm.Reds,\n",
        "            ),\n",
        "            cax=ax,\n",
        "            label='Edge Flow',\n",
        "        )\n",
        "\n",
        "    plt.suptitle(\"Smiley State Space\")\n",
        "\n",
        "\n",
        "def plot_state_space(model=None):\n",
        "    \"\"\"Get all of the unique possible states and their transitions, then plot.\"\"\"\n",
        "    enumerated_states, transitions = [], []\n",
        "    recursively_enumerate([], enumerated_states, transitions, only_valid=False)\n",
        "    unique_enumerated_states = get_unique(enumerated_states)\n",
        "    if isinstance(model, type(None)):\n",
        "        get_face_positions_and_plot(unique_enumerated_states, transitions)\n",
        "    else:\n",
        "        get_face_positions_and_plot(unique_enumerated_states, transitions, F_sa=F_sa)\n",
        "\n",
        "\n",
        "def face_hash(face):\n",
        "    \"\"\"Returns a binary hash for each submitted face.\"\"\"\n",
        "    return tuple([i in face for i in FEATURE_KEYS])\n",
        "\n",
        "\n",
        "def face_to_tensor(face, verbose=False):\n",
        "  \"\"\"Encodes a face as a binary tensor (converted to float32).\"\"\"\n",
        "  if verbose:\n",
        "      print(\"face={}, hash={}, tensor={}\".format(\n",
        "          face,\n",
        "          face_hash(face),\n",
        "          torch.tensor(face_hash(face)).float(),\n",
        "          )\n",
        "      )\n",
        "  return torch.tensor(face_hash(face)).float()\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Our feature space. A face has one mouth, one left eyebrow, and one right eyebrow.\n",
        "MOUTHS = [\"smile\", \"frown\"]\n",
        "LEFT_EB = [\"left_eb_down\", \"left_eb_up\"]\n",
        "RIGHT_EB = [\"right_eb_down\", \"right_eb_up\"]\n",
        "FEATURE_KEYS = MOUTHS + LEFT_EB + RIGHT_EB\n",
        "\n",
        "# Fixed hyperparameters.\n",
        "n_hid_units = 512\n",
        "n_episodes = 10_000\n",
        "learning_rate = 3e-3\n",
        "seed = 42\n",
        "\n",
        "print(\"For all experiments, our hyperparameters will be:\")\n",
        "print(\"    + n_hid_units={}\".format(n_hid_units))\n",
        "print(\"    + n_episodes={}\".format(n_episodes))\n",
        "print(\"    + learning_rate={}\".format(learning_rate))\n",
        "print(\"    + seed={}\".format(seed))"
      ],
      "metadata": {
        "id": "9i9IoXKsQNEC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv_6Adrni7c6"
      },
      "source": [
        "# Features: Faces, Both Happy and Sad\n",
        "\n",
        "For this simple example, we will generate ðŸ˜€ & ðŸ™ faces!\n",
        "\n",
        "Here, we define a face of being composed of two eyebrows (each can be up or down) and a mouth (smiling or frowning), for a total of 6 `FEATURES`:\n",
        "\n",
        "\n",
        "*   `smile`\n",
        "*   `frown`\n",
        "*   `left_eb_down`\n",
        "*   `left_eb_up`\n",
        "*   `right_eb_down`\n",
        "*   `right_eb_up`\n",
        "\n",
        "(where `eb` is shorthand for `eyebrow`). Above, we defined our feature space and some helper functions which we will use throughout the tutorial (we've also hidden some helper code for brevity)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, 5, figsize=(10, 50))\n",
        "plt.sca(ax[0])\n",
        "draw_face(['smile', 'left_eb_down', 'right_eb_down'], \"happy\")\n",
        "plt.sca(ax[1])\n",
        "draw_face(['frown', 'left_eb_up', 'right_eb_up'], \"sad\")\n",
        "plt.sca(ax[2])\n",
        "draw_face(['smile', 'frown', 'left_eb_up', 'right_eb_up'], \"mouth invalid\")\n",
        "plt.sca(ax[3])\n",
        "draw_face(['frown', 'left_eb_up', 'left_eb_down', 'right_eb_up'], \"eyebrow invalid\")\n",
        "plt.sca(ax[4])\n",
        "draw_face(['smile', 'left_eb_up'], \"face incomplete\")"
      ],
      "metadata": {
        "id": "4c64Iuws7dhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMzh41bW78l3"
      },
      "source": [
        "# The Reward Function\n",
        "\n",
        "A face is defined as a list of these keys. For example, above `['smile', 'left_eb_down', 'right_eb_down']` gives a happy face, and `['frown', 'left_eb_up', 'right_eb_up']` gives a sad face. Meanwhile other configurations give invalid faces, e.g., with multiple overlapping mouths or eyebrows, or not having two eyebrows in one mouth.\n",
        "\n",
        "If we want to train a GFN to sample faces, we first need to define a **reward function** which expresses the above intention. We want to only reward valid faces: those with two non-overlapping eyebrows and one mouth. If there are missing parts or two overlapping parts the reward is 0. Also, since it's better to be happy than sad, let's sample happy faces more often, by giving them a reward of 2, and sad faces a reward of 1.\n",
        "\n",
        "Since our trained sampler should respect $p(x) \\propto R(x)$, **at convergence** we should sample twice as many smiles as frowns! Note - we often don't *completely* converge without training for a long time, so we might not observe a $p(x)$ exactly proportional to $R(x)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9OqdXP69Pta"
      },
      "outputs": [],
      "source": [
        "def has_overlap(face: list):\n",
        "  \"\"\"\"Checks whether face contains either overlapping eyebrows or mouths.\"\"\"\n",
        "  if 'left_eb_down' in face and 'left_eb_up' in face:\n",
        "    return True\n",
        "\n",
        "  if 'right_eb_down' in face and 'right_eb_up' in face:\n",
        "    return True\n",
        "\n",
        "  if 'smile' in face and 'frown' in face:\n",
        "    return True\n",
        "\n",
        "  return False\n",
        "\n",
        "\n",
        "def face_reward(face):\n",
        "  \"\"\"Reward is 0 with incomplete/incorrect faces, 1 for sad, and 2 for happy.\"\"\"\n",
        "  # A face cannot have overlapping features.\n",
        "  if has_overlap(face):\n",
        "    return 0  # We will only very rarely sample an invalid face.\n",
        "\n",
        "  # A face must have exactly two eyebrows.\n",
        "  if sum([i in face for i in LEFT_EB + RIGHT_EB]) != 2:\n",
        "    return 0\n",
        "\n",
        "  if 'smile' in face:\n",
        "    return 2  # We want twice as many happy faces as sad faces.\n",
        "\n",
        "  if 'frown' in face:\n",
        "    return 1  # We want half as many happy sad faces as happy faces.\n",
        "\n",
        "  return 0  # If we reach this point, there's no mouth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_60a9KTZ9i85"
      },
      "source": [
        "# State Space\n",
        "\n",
        "Now that we've defined a reward function, let's take a look at what the state space looks like. We want to see all the possible routes to a valid terminal state.\n",
        "\n",
        "By state space, we mean the **space of intermediate constructions**, i.e. in this case this will include both partially created faces and fully created faces. For this tutorial we refer to these states as $s$, and to the full constructions as both $x$ and $s$ interchangeably (because $\\mathcal{X} \\subset \\mathcal{S}$).\n",
        "\n",
        "As you can see, we start with a base face that has just two eyes and we progressively add all possible patches that make a valid face. We call one such construction path from bottom to top a _trajectory_ or an _episode_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqlwrCmX9evc"
      },
      "outputs": [],
      "source": [
        "plot_state_space()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yynOkiks-DM0"
      },
      "source": [
        "The directed acyclic graph (DAG) above can be thought of as the unrolling of the `for` loop of the generation process, which allows us to visualize all possible paths. In Reinforcement Learning we'd call this the Markov Decision Process (MDP).\n",
        "\n",
        "What we're interested in is a _policy_, something that will tell us at each step what action to take (what patch to add). Since we are interested in learning a distribution, the policy itself will be a distribution over actions (possible patches to add) which is typically written $\\pi(a|s)$ in RL, but which we'll call $P_F$ ($F$ for forward) here. We'll call these arrows above, the $(s,s')$ pairs (or equivalently, $(s,a)$ pairs), _edges_, and the set of all edges $\\mathcal{E}$.\n",
        "\n",
        "The main idea behind GFlowNet is to interpret the DAG as a **flow network**. You could imagine each edge in the graph as a pipe through which some amount of water, or particles, _flows_. We then want to _find_ a flow where, (a) flow is preserved (meaning the amount of water going into a node should equal the amount of water going out of a node), and (b) the flow coming into a terminal state (a finished object) is equal to its reward (and in fact every non-terminal state _does not have any reward_).\n",
        "\n",
        "> \"Network\" here thus does _not_ refer to a _neural_ network architecture - it's not the \"Net\" in \"ConvNet\" - it refers to the MDP. So we use a GFlowNet, powered by a neural network, to discover the flows of the state space network. Naming is hard ðŸ˜‰.\n",
        "\n",
        "A central finding of the first GFlowNet paper is that if we match to each edge a _flow_, $F(s,s')>0$, and that this flow respects the following property:\n",
        "$$\\forall s', \\sum_{s: (s,s')\\in\\mathcal{E}} F(s,s') = R(s') + \\sum_{s'':(s',s'')\\in\\mathcal{E}} F(s',s'')$$\n",
        "which ensures _flow consistency_, AND we define the following policy\n",
        "$$P_F(s'|s) = \\frac{F(s, s')}{\\sum_{s''}F(s, s'')}$$\n",
        "which allows us to sample trajectories (paths through the DAG) using this policy, we will sample terminal states (finished objects $x$) with probability:\n",
        "\n",
        "$$p(x) \\propto R(x)$$.\n",
        "\n",
        "### Building and training a flow model with flow-matching\n",
        "\n",
        "We'll now setup a model and train it with the _flow-matching_ loss, introduced in [Flow Network based Generative Models for Non-Iterative Diverse Candidate Generation, Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, Yoshua Bengio](https://arxiv.org/abs/2106.04399)\n",
        "\n",
        "Now that we know we want to predict edge flows, we need to define some model which does that. We could use a model that is a function of a pair of states, but that's not too efficient (exercise left to the reader ;). Instead we'll take inspiration from RL and model a function of a single state which outputs multiple predictions at once, the flow for each possible child of that state.\n",
        "\n",
        "In other words, we train a model `F(state)` which outputs a vector which is indexed by the action used to reach a child of `state`, i.e. $F(s,s')$ is `F(s)[a]` if `a` leads from $s$ to $s'$ (e.g. `a` can be, \"add a raised eyebrow\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKuO8kjd-KeC"
      },
      "outputs": [],
      "source": [
        "class FlowModel(nn.Module):\n",
        "  def __init__(self, num_hid):\n",
        "    super().__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(6, num_hid),  # Input state is a binary vector representing feature\n",
        "                                # presence and/or absence.\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(num_hid, 6)  # Output 6 numbers for the 6 actions (child states).\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.mlp(x).exp()  # Flows must be positive, so we take the exponential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_dOaq7F4QrI"
      },
      "source": [
        "Another important thing we need to define is how we deal with parents. Since we'll be using the flow-matching objective to make the _flow consistency_ equation respected, we need to be able to enumerate the $(s, s')$ pairs that are \"parents\" of $s'$.\n",
        "\n",
        "In the left sum $$\\sum_{s: (s,s')\\in\\mathcal{E}} F(s,s') = R(s') + \\sum_{s'':(s',s'')\\in\\mathcal{E}} F(s',s'')$$\n",
        "the set $\\{s: (s,s')\\in\\mathcal{E}\\}$ is the set of parents of $s'$, while in the right sum, it is the set of children of $s'$. Since we're parameterizing $F$ with something that outputs a vector, computing the sum over children flows on the right is easy, we simply take `F(s').sum()`. But on the left, we need to do one forward pass for each parent to basically take `sum([F(ps)[pa] for ps, pa in parents(s')])` (we will see later in the tutorial that this is avoidable).\n",
        "\n",
        "Note - you might need `FEATURE_KEYS`, a variable containing the list of feature keys pre-defined for you, for the following exercises. This can be used to index the actions and states vectors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FEATURE_KEYS={}\".format(FEATURE_KEYS))"
      ],
      "metadata": {
        "id": "7r2VFKH8d85H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCP6HZit4Pqr"
      },
      "outputs": [],
      "source": [
        "def face_parents(state):\n",
        "  parent_states = []  # States that are parents of state.\n",
        "  parent_actions = []  # Actions that lead from those parents to state.\n",
        "\n",
        "  for feature in state:\n",
        "    # For each face part, there is a parent without that part.\n",
        "    parent_states.append(???)  # TODO: implement.\n",
        "\n",
        "    # The action to get there is the corresponding index of that face part.\n",
        "    parent_actions.append(???)  # TODO: implement.\n",
        "\n",
        "  return parent_states, parent_actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngrMxOMNi-Cz"
      },
      "source": [
        "Finally, to actually train the model we'll use the simplest differentiable loss possible, by just turning the above equation into a mean squared error (there are more efficient losses, but we will start with this as it is simplest):\n",
        "$$L(s') = \\left(\\sum_{s: (s,s')\\in\\mathcal{E}} F(s,s') - R(s') - \\sum_{s'':(s',s'')\\in\\mathcal{E}} F(s',s'')\\right)^2$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def flow_matching_loss(incoming_flows, outgoing_flows, reward):\n",
        "    \"\"\"Flow matching objective converted into mean squared error loss.\"\"\"\n",
        "    return ???  # TODO: implement."
      ],
      "metadata": {
        "id": "8CDBa8Rf40e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters\n",
        "\n",
        "Previously, we defined some hyperparameters that we will use for all following demonstrations:"
      ],
      "metadata": {
        "id": "Vz1bG57fUoWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "print(\"For all experiments, our hyperparameters will be:\")\n",
        "print(\"    + n_hid_units={}\".format(n_hid_units))\n",
        "print(\"    + n_episodes={}\".format(n_episodes))\n",
        "print(\"    + learning_rate={}\".format(learning_rate))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r_kFB8wgUnbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to instantiate and train the model. First, we let the model generate its own data by following its own policy (i.e., sampling actions based on edge flow predictions), and simply train on that data. Let's go through this code and then run it for a minute.\n",
        "\n",
        "Note: in the below, `state` is a list of strings representing the keys in `FEATURE_KEYS`, present on the face."
      ],
      "metadata": {
        "id": "4Q0qdo9146Zj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ot0qdZ-K4PsF"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "# Instantiate model n_hid_units optimizer\n",
        "F_sa = FlowModel(n_hid_units)\n",
        "opt = torch.optim.Adam(F_sa.parameters(), learning_rate)\n",
        "\n",
        "# To not complicate the code, I'll just accumulate losses here and take a\n",
        "# gradient step every `update_freq` episode (at the end of each trajectory).\n",
        "losses, sampled_faces = [], []\n",
        "minibatch_loss = 0\n",
        "update_freq = 4\n",
        "\n",
        "tbar = trange(n_episodes, desc=\"Training iter\")\n",
        "for episode in tbar:\n",
        "    state = []  # Each episode starts with an empty state.\n",
        "    edge_flow_preds = F_sa(face_to_tensor(state))  # Predict F(s, a).\n",
        "\n",
        "    for t in range(3):  # All trajectories are length 3 (not including s0).\n",
        "        # Normalizing gives us the probability of each action, from which we can\n",
        "        # sample actions to obtain the next state.\n",
        "\n",
        "        # Sample the action and compute the new state.\n",
        "        policy = ???  # TODO: Complete.\n",
        "        # We want to sample from the policy here (a Categorical distribution...)\n",
        "        action = ???  # TODO: Complete.\n",
        "        new_state = state + [FEATURE_KEYS[action]]\n",
        "\n",
        "        # To compute the loss, we'll first enumerate the parents, then compute\n",
        "        # the edge flows F(s, a) of each parent, indexing to get relevant flows.\n",
        "        parent_states, parent_actions = face_parents(new_state)\n",
        "        ps = torch.stack([face_to_tensor(p) for p in parent_states])\n",
        "        pa = torch.tensor(parent_actions).long()\n",
        "        parent_edge_flow_preds = F_sa(ps)[torch.arange(len(parent_states)), pa]\n",
        "\n",
        "        if t == 2:  # End of trajectory.\n",
        "            # We calculate the reward and set F(s,a) = 0 \\forall a, since there\n",
        "            # are no children of this state.\n",
        "            reward = ???  # TODO: To Complete.\n",
        "            edge_flow_preds = ???  # TODO: To Complete.\n",
        "        else:\n",
        "            # We compute F(s, a) and set the reward to zero.\n",
        "            reward = ???  # TODO: To Complete.\n",
        "            edge_flow_preds = ???  # TODO: To Complete.\n",
        "\n",
        "        minibatch_loss += flow_matching_loss(  # Accumulate.\n",
        "            parent_edge_flow_preds,\n",
        "            edge_flow_preds,\n",
        "            reward,\n",
        "        )\n",
        "        state = new_state  # Continue iterating.\n",
        "\n",
        "    # We're done with the episode, add the face to the list, and if we are at an\n",
        "    # update episode, take a gradient step.\n",
        "    sampled_faces.append(state)\n",
        "    if episode % update_freq == 0:\n",
        "\n",
        "        # Normalize accumulated loss.\n",
        "        minibatch_loss = minibatch_loss / (update_freq)\n",
        "        losses.append(minibatch_loss.item())\n",
        "        tbar.set_description(\"Training iter (loss={:.6f})\".format(minibatch_loss))\n",
        "        minibatch_loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        minibatch_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfiyp_UAjCd6"
      },
      "source": [
        "*We* now trained the model for a little while. It's not perfect but the loss seems low enough:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLppRbOk4Pua"
      },
      "outputs": [],
      "source": [
        "plot_loss_curve(losses, title=\"Loss over Training Iterations using the Flow Matching Objective\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g73O48sojKJi"
      },
      "source": [
        "Let's see what kind of faces the model is generating. Remember that we set the reward of smiling faces to 2 and the reward of frowny faces to 1. Since there are 4 possible smile and frown faces respectively (i.e. an equal number), we should expect there to be 2/3 smiling faces and 1/3 frowny faces when sampling from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JT1ZIR4Pw2"
      },
      "outputs": [],
      "source": [
        "check_sampled_faces(sampled_faces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W78EVVkjWkg"
      },
      "source": [
        "And we can visualize the edge flows. Note two things:\n",
        "\n",
        "\n",
        "\n",
        "*   Flows near the source node (initial state) are generally larger than terminal flows.\n",
        "*   The model has learned to ascribe low flows to invalid configurations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vy3vOaKjP0N"
      },
      "outputs": [],
      "source": [
        "plot_state_space(model=F_sa)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1fYebctjeGP"
      },
      "source": [
        "Well, the results look OK, but not great. We're still sampling invalid faces, and we're not sampling the correct proportion of happy to sad faces.\n",
        "\n",
        "One interesting side effect of training a GFlowNet model is that the flow of the initial state _is the partition function_. $F(s_0)$ is the sum of all rewards in the network (in the state space) since, in terms of flows, it is the unique source to all the sinks (which each sink in $R(x)$ flow).\n",
        "\n",
        "Since we have 4 terminal smiling states and 4 frowny ones with rewards 2 and 1 respectively, the total reward is $8 + 4 = 12$. Let's see if the model has learned that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcQY_JVF4E8l"
      },
      "outputs": [],
      "source": [
        "print(\"The final Z (partition function) estimate is {:.2f}\".format(\n",
        "    F_sa(face_to_tensor([])).sum())\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're heading in the right direction, but we aren't there yet.\n",
        "\n",
        "# Action Masking\n",
        "\n",
        "At this point you might be wondering: if we don't want our trained sampler to ever sample an invalid face, why should we allow it turing training? That's a big waste of time. You would be right. We can learn to not sample a particular face by providing a low reward for that face and training for long enough, but we can also simply disallow that face from being constructed in the first place by masking those actions during training. This greatly speeds up convergence, and is a common practice when using GFNs with reasonably challenging applications.\n",
        "\n",
        "We'll calculate a mask which we apply to the `edge_flow_preds` vector (our unnormalized distribution over next actions), such that we never sample a masked state. Otherwise, our training loop remains the same.\n",
        "\n",
        "Hint: You might need to know the `FEATURE_KEYS` here..."
      ],
      "metadata": {
        "id": "iqD-1gkI7DE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FEATURE_KEYS={}\".format(FEATURE_KEYS))"
      ],
      "metadata": {
        "id": "fPLhzIMsgRDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_forward_mask_from_state(state):\n",
        "    \"\"\"Here, we mask forward actions to prevent the selection of invalid configurations.\n",
        "        Recall our feature space is:\n",
        "            [smile, frown, left_eb_down, left_eb_up, right_eb_down, right_eb_up]\n",
        "        So we can only select one element from [0,1], [2,3], and [4,5].\n",
        "    \"\"\"\n",
        "    mask = np.ones(6)  # Allowed actions represented as 1, disallowed actions as 0.\n",
        "\n",
        "    # TODO: Complete logic:\n",
        "    ???\n",
        "\n",
        "    return torch.Tensor(mask).bool()"
      ],
      "metadata": {
        "id": "gQx7B-kQ9qYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's mask the `edge_flow_preds`..."
      ],
      "metadata": {
        "id": "I1VdVuYtnXte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed)\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "F_sa = FlowModel(n_hid_units)\n",
        "opt = torch.optim.Adam(F_sa.parameters(), learning_rate)\n",
        "\n",
        "# To not complicate the code, I'll just accumulate losses here and take a\n",
        "# gradient step every `update_freq` episode (at the end of each trajectory).\n",
        "losses_masking, sampled_faces = [], []\n",
        "minibatch_loss = 0\n",
        "update_freq = 4\n",
        "\n",
        "tbar = trange(n_episodes, desc=\"Training iter\")\n",
        "for episode in tbar:\n",
        "    state = []  # Each episode starts with an empty state.\n",
        "    edge_flow_preds = F_sa(face_to_tensor(state))  # Predict F(s, a).\n",
        "\n",
        "    for t in range(3):  # All trajectories are length 3 (not including s0).\n",
        "\n",
        "        # Here we mask the relevant forward actions.\n",
        "        mask = calculate_forward_mask_from_state(state)\n",
        "        edge_flow_preds = ???  # TODO: Complete.\n",
        "\n",
        "        # Normalizing gives us the probability of each action, from which we can\n",
        "        # sample actions to obtain the next state.\n",
        "        policy = edge_flow_preds / edge_flow_preds.sum()\n",
        "        action = Categorical(probs=policy).sample()\n",
        "        new_state = state + [FEATURE_KEYS[action]]\n",
        "\n",
        "        # To compute the loss, we'll first enumerate the parents, then compute\n",
        "        # the edge flows F(s, a) of each parent, indexing to get relevant flows.\n",
        "        parent_states, parent_actions = face_parents(new_state)\n",
        "        ps = torch.stack([face_to_tensor(p) for p in parent_states])\n",
        "        pa = torch.tensor(parent_actions).long()\n",
        "        parent_edge_flow_preds = F_sa(ps)[torch.arange(len(parent_states)), pa]\n",
        "\n",
        "        if t == 2:  # End of trajectory.\n",
        "            # We calculate the reward and set F(s,a) = 0 \\forall a, since there\n",
        "            # are no children of this state.\n",
        "            reward = face_reward(new_state)\n",
        "            edge_flow_preds = torch.zeros(6)\n",
        "        else:\n",
        "            # We compute F(s, a) and set the reward to zero.\n",
        "            reward = 0\n",
        "            edge_flow_preds = F_sa(face_to_tensor(new_state))\n",
        "\n",
        "        minibatch_loss += flow_matching_loss(  # Accumulate.\n",
        "            parent_edge_flow_preds,\n",
        "            edge_flow_preds,\n",
        "            reward,\n",
        "        )\n",
        "        state = new_state  # Continue iterating.\n",
        "\n",
        "    # We're done with the episode, add the face to the list, and if we are at an\n",
        "    # update episode, take a gradient step.\n",
        "    sampled_faces.append(state)\n",
        "    if episode % update_freq == 0:\n",
        "\n",
        "        # Normalize accumulated loss.\n",
        "        minibatch_loss = minibatch_loss / (update_freq)\n",
        "        losses_masking.append(minibatch_loss.item())\n",
        "        tbar.set_description(\"Training iter (loss={:.6f})\".format(minibatch_loss))\n",
        "        minibatch_loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        minibatch_loss = 0"
      ],
      "metadata": {
        "id": "Hhqf47ib7Sp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see this had a big effect on convergence:"
      ],
      "metadata": {
        "id": "S0iTuMkda1rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(\n",
        "    losses,\n",
        "    losses_B=losses_masking,\n",
        "    title=\"Loss over Training Iterations using the Flow Matching Objective\\nWith and Without Forward Masks\"\n",
        ")"
      ],
      "metadata": {
        "id": "-Axzwc-GBolh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can also see a big change in the sampling distribution. Namely, we are now sampling only valid configurations, and the sampling proportion is pretty close."
      ],
      "metadata": {
        "id": "BjIEiJbba01f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "check_sampled_faces(sampled_faces)"
      ],
      "metadata": {
        "id": "pds5ctnjGRI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And our estimate of the partition function is improved, but not yet there:"
      ],
      "metadata": {
        "id": "jtjBgVeHbGts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The final Z (partition function) estimate is {:.2f}\".format(\n",
        "    F_sa(face_to_tensor([])).sum()\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Rhi15fSFHCp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY4N4y-fjnpY"
      },
      "source": [
        "# Trajectory Balance\n",
        "\n",
        "[Trajectory Balance: Improved Credit Assignment in GFlowNets, Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, Yoshua Bengio](https://arxiv.org/abs/2201.13259)\n",
        "\n",
        "So far we've thought of flow-consistency very locally by considering a single state and looking at all the flow coming into that state and out of that state.\n",
        "\n",
        "Let's instead think about the flow _trajectories_ $\\tau$. If you think of all the possible paths from the initial state $s_0$ to some state $s$, each path can be \"assigned\" some amount of flow (just as we \"assigned\" flows to edges earlier). If we adopt this view, we can say we want all of the trajectory flows that go to a state to equal its reward (just like we wanted all incoming flows to a terminal state to be equal to its reward $R(x)$).\n",
        "\n",
        "$$F(s) = \\sum_{\\tau, s\\in\\tau} F(\\tau)$$\n",
        "\n",
        "We can also write the probability of a particular trajectory as the product of its policy steps:\n",
        "\n",
        "$$P(\\tau) = \\prod_{(s,s') \\in \\tau} P_F(s'|s) =\\frac{1}{Z}F(\\tau)$$.\n",
        "\n",
        "Note the relationship of $P(\\tau)$ with $F(\\tau)$, which asks \"what fraction of the total flow $Z$ goes through $\\tau$?\".\n",
        "\n",
        "We can equivilantly think of _backward_ trajectories using a backward policy $P_B$, which gives the distribution over the parents of a state, with the following property\n",
        "\n",
        "$$P(\\tau) = \\prod_{(s,s') \\in \\tau} P_B(s|s')$$\n",
        "\n",
        "A bit of rearranging (see paper), let $\\tau=(s_0,...,s_n=x)$ gives us this equality:\n",
        "\n",
        "$$Z \\prod_{t} P_F(s_{t+1}|s_t) = R(x)\\prod_t P_B(s_t|s_{t+1})$$\n",
        "\n",
        "\n",
        "Here's an intuitive view of what this means. Since all the incoming flow to a terminal state must be preserved, you can think of it being \"split up\" into different flows (one for each parent of that state). Since $P_B$ is a distribution, and therefore sums to 1, it does that splitting.\n",
        "\n",
        "> $R(x)\\prod_t P_B(s_t|s_{t+1})$ represents \"what fraction of the reward goes through this particular trajectory?\"\n",
        "\n",
        "The other side of this equation insteads splits up the partition function $Z$.\n",
        "\n",
        "> $Z \\prod_{t} P_F(s_{t+1}|s_t)$ represents \"what fraction of the partition function goes through this trajectory?\"\n",
        "\n",
        "and the equality enforces that those two quantities are equal.\n",
        "\n",
        "## Implementing Trajectory Balance\n",
        "\n",
        "Since we assume $R(x)$ is given, we'll now need to estimate $Z$, $P_B$ and $P_F$. In the last exercise, we modeled $F(s,a)$ directly, but as the number of states grows, this is not ideal. Instead, implementations of GFlowNet typically model $\\log F(s, a)$ (just like we typically train classifiers with a logsoftmax rather than predicting the probabilities directly). We will do the same thing here and actually estimate $\\log Z$ as well as the logits of $P_B$ and $P_F$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezWPUujDjkg8"
      },
      "outputs": [],
      "source": [
        "class TBModel(nn.Module):\n",
        "  def __init__(self, num_hid):\n",
        "    super().__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(6, num_hid),  # 6 input features.\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(num_hid, 12),  # 12 outputs: 6 for P_F and 6 for P_B.\n",
        "    )\n",
        "    self.logZ = nn.Parameter(torch.ones(1))  # log Z is just a single number.\n",
        "\n",
        "  def forward(self, x):\n",
        "    logits = self.mlp(x)\n",
        "    # Slice the logits into forward and backward policies.\n",
        "    P_F = logits[..., :6]\n",
        "    P_B = logits[..., 6:]\n",
        "\n",
        "    return P_F, P_B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bNPW0fjkFm3"
      },
      "source": [
        "Just like we turned the flow-matching equation into a MSE loss, we will use the following loss for a trajectory:\n",
        "$$L(\\tau) = \\left(\\log \\frac{Z_\\theta \\prod_t P_F(s_{t+1}|s_t;\\theta)}{R(x)\\prod_t P_B(s_t|s_{t+1}; \\theta)} \\right)^2$$\n",
        "\n",
        "or equivalently\n",
        "$$L(\\tau) = \\left(\\log Z_\\theta + \\sum_t \\log P_F(s_{t+1}|s_t;\\theta) - \\log R(x) - \\sum_t \\log P_B(s_t|s_{t+1}; \\theta) \\right)^2$$\n",
        "\n",
        "You may notice that in this loss there is no need to compute parents! In the current toy example this won't make a difference, but when generating complex objects this saves us O(average number of parents) forward passes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trajectory_balance_loss(logZ, log_P_F, log_P_B, reward):\n",
        "    \"\"\"Trajectory balance objective converted into mean squared error loss.\"\"\"\n",
        "    return ???  # TODO: Complete."
      ],
      "metadata": {
        "id": "fdvANn9RXFFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need, for the backward policy, to be able to calculate valid backward actions:"
      ],
      "metadata": {
        "id": "Alf3y8PFoho9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_backward_mask_from_state(state):\n",
        "    \"\"\"Here, we mask backward actions to only select parent nodes.\"\"\"\n",
        "    # This mask should be 1 for any action that could have led to the current state,\n",
        "    # otherwise it should be zero.\n",
        "    return torch.Tensor(\n",
        "        [1 if feature in state else 0 for feature in FEATURE_KEYS]\n",
        "    ).bool()"
      ],
      "metadata": {
        "id": "4WiUALPNA2Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again we will roll out trajectories, then compute the loss after each trajectory and take gradient steps at some frequency. Let's train the model for a minute."
      ],
      "metadata": {
        "id": "ROU2rrWnXKts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUC059-SkCrP"
      },
      "outputs": [],
      "source": [
        "set_seed(seed)\n",
        "\n",
        "# Instantiate model and optimizer\n",
        "model = TBModel(n_hid_units)\n",
        "opt = torch.optim.Adam(model.parameters(),  learning_rate)\n",
        "\n",
        "# To not complicate the code, I'll just accumulate losses here and take a\n",
        "# gradient step every `update_freq` episode (at the end of each trajectory).\n",
        "losses, sampled_faces, logZs = [], [], []\n",
        "minibatch_loss = 0\n",
        "update_freq = 4\n",
        "\n",
        "for episode in tqdm(range(n_episodes), ncols=40):\n",
        "    state = []  # Each episode starts with an empty state.\n",
        "    P_F_s, P_B_s = model(face_to_tensor(state))  # Forward and backward policy\n",
        "    total_log_P_F, total_log_P_B = 0, 0\n",
        "\n",
        "    for t in range(3):  # All trajectories are length 3 (not including s0).\n",
        "\n",
        "        # Here we mask the relevant forward actions.\n",
        "        mask = calculate_forward_mask_from_state(state)\n",
        "        P_F_s = torch.where(mask, P_F_s, -100)  # Removes invalid forward actions.\n",
        "\n",
        "        # TO IMPLEMENT w hints.\n",
        "        # Here P_F is logits, so we use Categorical to compute a softmax.\n",
        "        categorical = Categorical(logits=P_F_s)\n",
        "        action = categorical.sample()\n",
        "        new_state = state + [FEATURE_KEYS[action]]  # \"Go\" to next state.\n",
        "        total_log_P_F += categorical.log_prob(action)  # Accumulate the log_P_F sum.\n",
        "\n",
        "        if t == 2:  # End of trajectory.\n",
        "            reward = torch.tensor(face_reward(new_state)).float()\n",
        "\n",
        "        # We recompute P_F and P_B for new_state.\n",
        "        P_F_s, P_B_s = model(face_to_tensor(new_state))\n",
        "        mask = calculate_backward_mask_from_state(new_state)\n",
        "        P_B_s = torch.where(mask, P_B_s, -100)  # Removes invalid backward actions.\n",
        "\n",
        "        # Accumulate P_B, going backwards from `new_state`. We're also just\n",
        "        # going to use opposite semantics for the backward policy. I.e., for\n",
        "        # P_F action `i` added feature `i`, so for P_B action `i` removes\n",
        "        # feature `i`, this way we can just keep the same indices.\n",
        "        # TO IMPLEMENT w hints.\n",
        "        total_log_P_B += Categorical(logits=P_B_s).log_prob(action)\n",
        "\n",
        "        state = new_state  # Continue iterating.\n",
        "\n",
        "    # We're done with the trajectory, let's compute its loss. Since the reward\n",
        "    # can sometimes be zero, instead of log(0) we'll clip the log-reward to -20.\n",
        "    minibatch_loss += trajectory_balance_loss(\n",
        "        model.logZ,\n",
        "        total_log_P_F,\n",
        "        total_log_P_B,\n",
        "        reward,\n",
        "    )\n",
        "\n",
        "    # We're done with the episode, add the face to the list, and if we are at an\n",
        "    # update episode, take a gradient step.\n",
        "    sampled_faces.append(state)\n",
        "    if episode % update_freq == 0:\n",
        "        losses.append(minibatch_loss.item())\n",
        "        logZs.append(model.logZ.item())\n",
        "        minibatch_loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        minibatch_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJbkZyIqkO7n"
      },
      "source": [
        "Once again we can plot the loss and see that it's generally going down. We could do much better in terms of hyperparameters or learning rate scheduling, but for the purposes of this tutorial we'll leave it at that.\n",
        "\n",
        "Another thing we can see is that the value of $Z$ we're estimating reaches around 12 once again, confirming that the model \"learned about\" all possible rewards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbHGmNz6kLqx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 1, figsize=(10, 6))\n",
        "plt.sca(ax[0])\n",
        "plt.plot(losses, color=\"black\")\n",
        "plt.yscale('log')\n",
        "plt.ylabel('Loss')\n",
        "plt.sca(ax[1])\n",
        "plt.plot(np.exp(logZs), color=\"black\")\n",
        "plt.ylabel('Estimated Z');\n",
        "plt.xlabel('Step')\n",
        "plt.suptitle(\"Loss and Estimated Partition Function for the Trajectory Balance Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtP4TiKxkVoJ"
      },
      "source": [
        "This is the final value we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRktx2rWkUzL"
      },
      "outputs": [],
      "source": [
        "print(\"The final Z (partition function) estimate is {:.2f}\".format(\n",
        "    model.logZ.exp().item())\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kbvp88gkbvJ"
      },
      "source": [
        "And we can once again check that we get about 2/3 of smiley samples and only valid samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "splrw97YkTFl"
      },
      "outputs": [],
      "source": [
        "check_sampled_faces(sampled_faces)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsmyL4cFZj81"
      },
      "source": [
        "# `torchgfn` (Optional)\n",
        "\n",
        "Now that we've walked through how to sample happy faces twice as often as frowny faces, let's look at how to implement this using the lightweight `torchgfn` library.\n",
        "\n",
        "We'll duplicate the relevant logic from above, to create self-contained examples below (just make sure you run the first two cells of this notebook to add the features). First, let's set up the python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN0u6hy7Q3kj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from torchtyping import TensorType as TT\n",
        "from typing import ClassVar, Tuple, cast\n",
        "\n",
        "import gfn\n",
        "from gfn.actions import Actions\n",
        "from gfn.env import DiscreteEnv\n",
        "from gfn.gflownet import FMGFlowNet, TBGFlowNet\n",
        "from gfn.modules import DiscretePolicyEstimator, ScalarEstimator\n",
        "from gfn.preprocessors import IdentityPreprocessor\n",
        "from gfn.states import DiscreteStates\n",
        "from gfn.utils.modules import NeuralNet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `torchgfn` Smile Environment\n",
        "\n",
        "First, we need to define an environment for our `gflownet` to interact with. Face construction (at least, as defined here) is a discrete action space, so we will leverage the `gfn.env.DiscreteEnv` base class, as well as the `Actions` class.\n",
        "\n",
        "Each environment in torchgfn must generate an instance of the `States` class, which (in the discrete case) uses `masks` to define allowable actions, define whether the trajectory is complete, and backward actions to reach parent states. In `torchgfn` the forward action space (which in our case is the addition of 1 out of 6 facial features) has an additional *exit action*, which the framework uses to end a trajectory. The sampling process continues until all batch elements have exited. In our case, all trajectories are exactly length 3, so this is simple: we mask out the exit action until 3 non-exit actions have been selected, and then we mask all actions *except* exit to ensure the trajectory terminates.\n",
        "\n",
        "We must also define the `s0` (initial) and `sf` (final) state. In this case, `s0` is simply all zero, and `sf` is all `-1` (but it could be anything that does not look like a normal state tensor).\n",
        "\n",
        "We also define the `reward` using vectorized logic. Note that `torchgfn` uses `log_rewards()` internally, and since we have a reward of `0`, we must clip the `log_reward` (since `log(0) = -inf`). This clipping is handled automatically for us, and the clipping can be tuned using the `log_reward_clip` flag if so desired."
      ],
      "metadata": {
        "id": "4Rm5G7ZLV02g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92RxW4V7aLk7"
      },
      "outputs": [],
      "source": [
        "class FacesEnv(DiscreteEnv):\n",
        "    def __init__(self, log_reward_clip=-100, mask_invalid_actions=True):\n",
        "        \"\"\"Faces environment. States are represented as 6-element binary tensors.\n",
        "\n",
        "        All trajectories are enforced to be length 3 using states.forward_masks.\n",
        "        \"\"\"\n",
        "        self.feature_keys = [\n",
        "            'left_eb_down',\n",
        "            'left_eb_up',\n",
        "            'right_eb_down',\n",
        "            'right_eb_up',\n",
        "            'smile',\n",
        "            'frown',\n",
        "        ]\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        self.log_reward_clip = log_reward_clip\n",
        "        self.mask_invalid_actions = mask_invalid_actions\n",
        "\n",
        "        state_dim = len(self.feature_keys)\n",
        "        n_actions = state_dim + 1  # all 6 face elements + 1 exit action.\n",
        "\n",
        "        super().__init__(\n",
        "            n_actions=n_actions,\n",
        "            # We start with an empty face.\n",
        "            s0=torch.zeros(state_dim, dtype=torch.float, device=self.device),\n",
        "            # Sf represents when a trajectory is done (we selected the exit action).\n",
        "            sf=torch.ones(state_dim, dtype=torch.float, device=self.device) * -1,\n",
        "            device_str=\"cpu\",\n",
        "            # These are sometimes handy to generate tensors. In this case, not needed.\n",
        "            preprocessor=IdentityPreprocessor(output_dim=state_dim)\n",
        "        )\n",
        "\n",
        "    def make_States_class(self) -> type[DiscreteStates]:\n",
        "        \"Creates a States class for this environment\"\n",
        "        env = self\n",
        "\n",
        "        class FaceStates(DiscreteStates):\n",
        "            state_shape: ClassVar[tuple[int, ...]] = (env.n_actions - 1,)  # this is 6.\n",
        "            s0 = env.s0   # this is 6 zeros.\n",
        "            sf = env.sf   # this is 6 -1's.\n",
        "            n_actions = env.n_actions  # this is 7. 6 features, plus 1 for exit.\n",
        "            device = env.device\n",
        "\n",
        "            def update_masks(self) -> None:\n",
        "                \"Update the masks based on the current states.\"\n",
        "                # Backward masks are simply any action we've already taken.\n",
        "                self.backward_masks = self.tensor != 0  # n - 1 actions.\n",
        "\n",
        "                # Forward masks begin as allowing any action. Allowed actions are 1.\n",
        "                self.init_forward_masks(set_ones=True)\n",
        "\n",
        "                # Then, we remove any done action, and also the exit action.\n",
        "                self.set_nonexit_masks(self.tensor == 1, allow_exit=False)\n",
        "\n",
        "                if env.mask_invalid_actions:\n",
        "                    # Now we remove invalid actions. Here we are enforcing that\n",
        "                    # only one left eyebrow, one right eyebrow, and one smile can be\n",
        "                    # selected. 0 = not allowed.\n",
        "                    invalid_actions = torch.ones(self.forward_masks.shape).bool()\n",
        "                    invalid_actions[..., 0][self.tensor[..., 1].bool()] = 0  # l_eb\n",
        "                    invalid_actions[..., 1][self.tensor[..., 0].bool()] = 0  # l_eb\n",
        "                    invalid_actions[..., 2][self.tensor[..., 3].bool()] = 0  # r_eb\n",
        "                    invalid_actions[..., 3][self.tensor[..., 2].bool()] = 0  # r_eb\n",
        "                    invalid_actions[..., 4][self.tensor[..., 5].bool()] = 0  # smile\n",
        "                    invalid_actions[..., 5][self.tensor[..., 4].bool()] = 0  # smile\n",
        "\n",
        "                    self.forward_masks = (self.forward_masks * invalid_actions)\n",
        "\n",
        "                # Trajectories must be length 3. Any trajectory that has taken 3 actions\n",
        "                # should be forced to exit.\n",
        "                batch_idx = self.tensor.sum(-1) >= 3\n",
        "                self.set_exit_masks(batch_idx)\n",
        "\n",
        "        return FaceStates\n",
        "\n",
        "    def maskless_step(\n",
        "        self, states: DiscreteStates, actions: Actions\n",
        "    ) -> TT[\"batch_shape\", \"state_shape\", torch.float]:\n",
        "        return states.tensor.scatter(-1, actions.tensor, 1, reduce=\"add\")\n",
        "\n",
        "    def maskless_backward_step(\n",
        "        self, states: DiscreteStates, actions: Actions\n",
        "    ) -> TT[\"batch_shape\", \"state_shape\", torch.float]:\n",
        "        return states.tensor.scatter(-1, actions.tensor, -1, reduce=\"add\")\n",
        "\n",
        "    def reward(self, states: DiscreteStates) -> TT[\"batch_shape\", torch.float]:\n",
        "        \"\"\"The face reward is calculated as:\n",
        "            + 0 if the face is invalid (overlapping features OR not satisfying\n",
        "                the constraint of 2 eyebrows and 1 mouth).\n",
        "                + This will only be possible if `mask_invalid_actions` is False.\n",
        "            + 1 if the face is frowny. :(\n",
        "            + 2 if the face is smiley. :)\n",
        "        \"\"\"\n",
        "        if not env.mask_invalid_actions:\n",
        "            # Tensor organization is [left_eb *2, right_eb * 2, mouth * 2]\n",
        "            valid = torch.zeros(states.batch_shape + (3,))\n",
        "            valid[..., 0] = states.tensor[..., :2].sum(-1) == 1  # One left eyebrow.\n",
        "            valid[..., 1] = states.tensor[..., 2:4].sum(-1) == 1  # One right eyebrow.\n",
        "            valid[..., 2] = states.tensor[..., 4:].sum(-1) == 1  # One mouth.\n",
        "            valid = valid.prod(-1, keepdim=True)  #  Two eyebrows, one mouth.\n",
        "\n",
        "        # Add the emotion rewards.\n",
        "        rewards = torch.zeros(states.batch_shape + (1,))\n",
        "        rewards[states.tensor[..., 4] == 1] = torch.tensor([2])  # Smiles.\n",
        "        rewards[states.tensor[..., 5] == 1] = torch.tensor([1])  # Frowns.\n",
        "\n",
        "        if not env.mask_invalid_actions:\n",
        "            rewards = rewards * valid  # This will remove any double mouths.\n",
        "\n",
        "        return rewards.squeeze()\n",
        "\n",
        "env = FacesEnv()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A `torchgfn` Training Loop\n",
        "\n",
        "In this simple example, we use the environment's `States` class to keep track of our terminal states (our sampled faces). Our `gflownet` allows us to sample batches of trajectories, and allows us to calculate the loss using the `env` (which contains our reward function) and `samples`."
      ],
      "metadata": {
        "id": "Kg6xAWT2Qu1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(gflownet, optimizer, env, batch_size = 128, n_episodes = 25_000):\n",
        "    \"\"\"Training loop, keeping track of terminal states over training.\"\"\"\n",
        "    # This stores example terminating states.\n",
        "    visited_terminating_states = env.States.from_batch_shape((0,))\n",
        "    states_visited = 0\n",
        "    losses = []\n",
        "\n",
        "    for iteration in trange(n_episodes // batch_size):\n",
        "        trajectories = gflownet.sample_trajectories(env, n_samples=batch_size)\n",
        "        samples = gflownet.to_training_samples(trajectories)\n",
        "        optimizer.zero_grad()\n",
        "        loss = gflownet.loss(env, samples)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        visited_terminating_states.extend(trajectories.last_states)\n",
        "        states_visited += len(trajectories)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return visited_terminating_states, states_visited, losses"
      ],
      "metadata": {
        "id": "z3w9PQU0QyIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flow Matching with `torchgfn`\n",
        "\n",
        "The details of the Flow Matching objective are handled within the `FMGFlowNet` class, which requires us to provide it with an `estimator`, which is simply a neural network that accepts a state and produces logits over next actions. Under the hood, all `gflownets` in `torchgfn` inherit from `nn.Module` and function as expected (e.g., see `gflownet.parameters()`).\n",
        "\n",
        "Note that for numerical stability reasons, we estimate the log edge flows here, not the edge flows as we did in the above example."
      ],
      "metadata": {
        "id": "meNtpO0VCxsS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MZpRVULBp5r"
      },
      "outputs": [],
      "source": [
        "# nn.Module that estimates _log_ edge flows.\n",
        "module = NeuralNet(\n",
        "    input_dim=env.preprocessor.output_dim,\n",
        "    output_dim=env.n_actions,\n",
        "    hidden_dim=n_hid_units,\n",
        "    n_hidden_layers=1,\n",
        ")\n",
        "# This is our _log_ edge flow estimator.\n",
        "estimator = DiscretePolicyEstimator(\n",
        "    module=module,\n",
        "    n_actions=env.n_actions,\n",
        "    preprocessor=env.preprocessor,\n",
        ")\n",
        "\n",
        "# The gflownet class wraps our estimator (inclusing sampler functionality).\n",
        "gflownet = FMGFlowNet(estimator)\n",
        "optimizer = torch.optim.Adam(gflownet.parameters(), lr=learning_rate)\n",
        "\n",
        "visited_terminating_states, states_visited, losses = train(\n",
        "    gflownet,\n",
        "    optimizer,\n",
        "    env,\n",
        "    n_episodes=n_episodes * 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now see that our flow matching network is sampling faces as expected:"
      ],
      "metadata": {
        "id": "BU8M3lWLXQU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vei7EVHe7Vj2"
      },
      "outputs": [],
      "source": [
        "analyze_torchgfn_results(visited_terminating_states, env)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And our loss curve is reasonable:"
      ],
      "metadata": {
        "id": "L9RQOhynW3fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(losses, title=\"Loss Curve for Flow Matching Objective using TorchGFN\")"
      ],
      "metadata": {
        "id": "4Xg_IPaHWvZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the flows at $S_0$, we first need to make an instance of the env's `States` class, and then pass that class an all-zero (empty) state. Finally, this $S_0$ states object can be passed to our trained estimator (estimators are wrappers for `nn.Modules`,\n",
        "which know to handle `States` classes).\n"
      ],
      "metadata": {
        "id": "MjvrVwpZqNfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that here, we have the log edge flows, so we take the sum(exp(log_flows)) to\n",
        "# calculate the partition function estimate.\n",
        "s_0 = env.make_States_class()(torch.zeros(6))\n",
        "print(\"Partition function estimate Z={:.2f}\".format(\n",
        "    sum(torch.exp(estimator(s_0)[:6]))  # logsumexp.\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "A5Kq52J0Ipoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trajectory Balance with `torchgfn`\n",
        "\n",
        "Similarly, we can train a gflownet using Trajectory Balance using the `TBGFlowNet` class. Unlike before, we separately parameterize the forward and backward policies are two different `estimators`, which are passed to the `TBGFlowNet`. In this example we don't use a replay buffer, so we set `on_policy=True`.\n",
        "\n",
        "One common trick with trajectory balance is to learn the `logZ` parameter with a higher learning rate than the rest of the network."
      ],
      "metadata": {
        "id": "XoTopzlxCmgP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6P1Z4QcbBfPE"
      },
      "outputs": [],
      "source": [
        "# nn.Modules for the forward and backward policy estimators.\n",
        "pf_module = NeuralNet(\n",
        "    input_dim=env.preprocessor.output_dim,\n",
        "    output_dim=env.n_actions,\n",
        "    hidden_dim=n_hid_units,\n",
        "    n_hidden_layers=1,\n",
        ")\n",
        "pb_module = NeuralNet(\n",
        "    input_dim=env.preprocessor.output_dim,\n",
        "    output_dim=env.n_actions - 1,\n",
        "    hidden_dim=n_hid_units,\n",
        "    n_hidden_layers=1,\n",
        ")\n",
        "# Estimators for the forward and backward policies.\n",
        "pf_estimator = DiscretePolicyEstimator(\n",
        "    module=pf_module,\n",
        "    n_actions=env.n_actions,\n",
        "    preprocessor=env.preprocessor,\n",
        ")\n",
        "pb_estimator = DiscretePolicyEstimator(\n",
        "    module=pb_module,\n",
        "    n_actions=env.n_actions,\n",
        "    is_backward=True,\n",
        "    preprocessor=env.preprocessor,\n",
        ")\n",
        "\n",
        "# Our trajectory balance gflownet accepts both policy estimators.\n",
        "gflownet = TBGFlowNet(\n",
        "    pf=pf_estimator,\n",
        "    pb=pb_estimator,\n",
        "    on_policy=True,  # No replay buffer.\n",
        ")\n",
        "\n",
        "# Policy parameters recieve one LR, and LogZ gets a dedicated, typically higher LR.\n",
        "params = [\n",
        "    {\"params\": [\n",
        "        v for k, v in dict(gflownet.named_parameters()).items() if k != \"logZ\"\n",
        "        ],\n",
        "     \"lr\": learning_rate,\n",
        "    }\n",
        "]\n",
        "params.append(\n",
        "    {\"params\": [\n",
        "        dict(gflownet.named_parameters())[\"logZ\"]\n",
        "        ],\n",
        "     \"lr\": learning_rate * 10,\n",
        "     }\n",
        ")\n",
        "optimizer = torch.optim.Adam(params)\n",
        "\n",
        "visited_terminating_states, states_visited, losses = train(\n",
        "    gflownet,\n",
        "    optimizer,\n",
        "    env,\n",
        "    n_episodes=n_episodes * 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_torchgfn_results(visited_terminating_states, env)"
      ],
      "metadata": {
        "id": "miDx7913Bm4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(losses, title=\"Loss Curve for Trajectory Balance Objective using TorchGFN\")"
      ],
      "metadata": {
        "id": "ZrUZbfjmW8-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The partition function estimate is Z={:.2f}\".format(\n",
        "    torch.exp(gflownet.logZ).item()\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "9gybXbsWIa_t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}